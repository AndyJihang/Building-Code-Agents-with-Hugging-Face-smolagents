{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSs3HrZfMn3Qq6Y16B9VDa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndyJihang/Building-Code-Agents-with-Hugging-Face-smolagents/blob/main/Build_a_deep_research_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Research Agent (Smolagents)\n",
        "\n",
        "This Colab builds a multi-step web research agent with Smolagents that answers technical questions and returns a concise synthesis with citations.\n",
        "\n",
        "## What it does?\n",
        " - Wraps an LLM (OpenAI via OpenAIServerModel, or HF) as the agent’s brain.\n",
        " - Adds two web tools: web_search (discover sources) and visit_webpage (fetch + convert HTML→Markdown, trim, handle errors/timeouts).\n",
        " - Orchestrates a loop: plan → search → fetch → extract → dedupe → rank → synthesize → cite.\n",
        " - Produces a ~N-word answer plus 3+ credible sources.\n",
        "## Quality & safety\n",
        " - URL de-duplication, relevance scoring, and cap on pages per query.\n",
        " - Simple allow/deny lists to avoid low-quality or NSFW domains; retries + timeouts.\n",
        " - Configurable knobs: MAX_SOURCES, MAX_PAGES_PER_SOURCE, MAX_STEPS, MAX_TOKENS."
      ],
      "metadata": {
        "id": "djdWIUp8jiS1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKYs1eRchbwl",
        "outputId": "e4ae268a-8fb0-46e2-972e-3c2be10760cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# 1) Install\n",
        "!pip -q install \"smolagents[openai,toolkit]\" duckduckgo-search markdownify requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Imports & model\n",
        "import os, json, re, requests\n",
        "from markdownify import markdownify\n",
        "from smolagents import ToolCallingAgent, OpenAIServerModel, DuckDuckGoSearchTool, tool\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "model = OpenAIServerModel(\n",
        "    model_id=\"gpt-4o-mini\",        # or gpt-4o\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    temperature=0.2,\n",
        ")"
      ],
      "metadata": {
        "id": "TjSlLcnEiI2N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Page-reading tool (HTML -> compact Markdown)\n",
        "@tool\n",
        "def visit_webpage(url: str) -> str:\n",
        "    \"\"\"Visits a webpage and returns cleaned Markdown.\n",
        "    Args:\n",
        "        url: page URL to fetch\n",
        "    \"\"\"\n",
        "    try:\n",
        "        html = requests.get(url, timeout=20).text\n",
        "        md = markdownify(html).strip()\n",
        "        # compact newlines and remove super-long lines\n",
        "        md = re.sub(r\"\\n{3,}\", \"\\n\\n\", md)\n",
        "        return md[:40000]\n",
        "    except Exception as e:\n",
        "        return f\"[visit_webpage error] {e}\"\n",
        "\n",
        "search = DuckDuckGoSearchTool()  # built-in search tool\n",
        "\n",
        "# 4) “Deep researcher” instructions (citations + iteration rules)\n",
        "INSTRUCTIONS = \"\"\"\n",
        "You are DeepResearcher.\n",
        "- Plan -> search -> open pages -> extract key points -> iterate if needed.\n",
        "- Always provide a concise final answer with a 'SOURCES:' section listing the exact URLs you used.\n",
        "- Prefer diverse sources. Avoid duplicates (same domain) unless necessary.\n",
        "- Quote sparingly; summarize in your own words.\n",
        "- If uncertain, say what’s uncertain and what further search you’d do.\n",
        "\"\"\"\n",
        "\n",
        "agent = ToolCallingAgent(\n",
        "    tools=[search, visit_webpage],\n",
        "    model=model,\n",
        "    instructions=INSTRUCTIONS,\n",
        "    max_steps=10,   # raise for deeper dives\n",
        ")\n",
        "\n",
        "# 5) Run an example query\n",
        "q = \"Summarize the main differences between FlashAttention and xFormers attention. 150 words. Include 3 sources.\"\n",
        "print(agent.run(q))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9YbB3--HiNen",
        "outputId": "0a0e61ce-b3ae-4d7e-b936-58cf51f78c84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mSummarize the main differences between FlashAttention and xFormers attention. 150 words. Include 3 sources.\u001b[0m     \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m OpenAIServerModel - gpt-4o-mini \u001b[0m\u001b[38;2;212;183;2m──────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Summarize the main differences between FlashAttention and xFormers attention. 150 words. Include 3 sources.</span>     <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ OpenAIServerModel - gpt-4o-mini ───────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ Calling tool: 'web_search' with arguments: {'query': 'FlashAttention vs xFormers attention differences'}        │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ Calling tool: 'web_search' with arguments: {'query': 'FlashAttention vs xFormers attention differences'}        │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ Calling tool: 'web_search' with arguments: {'query': 'FlashAttention overview'}                                 │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ Calling tool: 'web_search' with arguments: {'query': 'FlashAttention overview'}                                 │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ Calling tool: 'web_search' with arguments: {'query': 'xFormers attention overview'}                             │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ Calling tool: 'web_search' with arguments: {'query': 'xFormers attention overview'}                             │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Observations: ## Search Results\n",
              "\n",
              "|최신 한국녀 포르노 영상들, #2 - xHamster](https://ko.xhamster2.com/categories/korean/newest/2)\n",
              "27 June 2025 오류 로딩 썸네일 08:12 후장 섹스로 오르가즘을 느끼는 한국 아시아 소녀 후장 RealDomSpank 3.9K views 18 \n",
              "June 2025\n",
              "\n",
              "|/todays-selection | 성인 야동 사이트 - 아라곤19 | aragon19.org](https://aragon19.org/)\n",
              "/todays-selection - 아라곤19 에서 제공하는 무료야동 및 성인 야동사이트로 일본야동 서양야동 동양야동 한국야동 \n",
              "동영상을 실시간 업데이트 합니다.\n",
              "\n",
              "|KBJFan - Watch Free Korean BJ Online](https://www.kbjfan.com/)\n",
              "KBJFan provides Korean BJ live stream and recorded videos. We update Korean BJ record videos every day. Enjoy Free \n",
              "Korean BJ shows online without ads!\n",
              "\n",
              "|야코주소 (YAKO) | 무료성인사이트, 한국야동](https://www.yako.gg/best?cate=korea)\n",
              "야코주소 (YAKO)는 실시간으로 한국야동과 일본야동, 최신야동, 성인야동을 무료로 시청하실 수 있는 무료 성인사이트 \n",
              "입니다.\n",
              "\n",
              "|국산 Porn Videos | Pornhub.com](https://www.pornhub.com/video/search?search=국산)\n",
              "국산 Porn Videos chinese hd sex sex machine 한국 야동 asian korean 한국 야동 국산 야동 onlyfans korean korean \n",
              "couple 한국 섹스 한국 bj 국산 야동 japanese 허누나 korean porn 한국 …\n",
              "\n",
              "|YouTube Korea - YouTube](https://www.youtube.com/@youtubekorea)\n",
              "YouTube Korea 공식 채널에서는 다양한 종류의 콘텐츠를 소개하고 있습니다. 채널 구독을 통해 국내외에서 화제가 되고 \n",
              "있는 최신 동영상을 확인해보세요.\n",
              "\n",
              "|무료 한국인 비제이 포르노 비디오 | xHamster](https://ko.xhamster2.com/tags/korean-bj)\n",
              "xHamster에서 한국인 비제이 포르노 비디오를 확인하세요. 지금 모든 한국인 비제이 XXX 비디오를 시청하세요!\n",
              "\n",
              "|한국녀 포르노 영상들, #3 - xHamster](https://ko.xhamster2.com/categories/korean/3)\n",
              "8.2M views 오류 로딩 썸네일 07:01 내 한국 의붓여동생의 보지가 너무 완벽해, 나는 그것을 쳐서 사정해 OniSoi 81.7K \n",
              "views 오류 로딩 썸네일\n",
              "\n",
              "|야코주소 (YAKO) | 무료성인사이트, 한국야동](https://www.yako.gg/all?page=2)\n",
              "야동코리아(YAKO)는 실시간으로 한국야동과 일본야동, 최신야동, 성인야동을 무료로 시청하실 수 있는 무료 성인사이트 \n",
              "입니다.\n",
              "\n",
              "|야코주소 (YAKO) | 무료성인사이트, 한국야동](https://www.yako.gg/best?cate=all)\n",
              "야코주소 (YAKO)는 실시간으로 한국야동과 일본야동, 최신야동, 성인야동을 무료로 시청하실 수 있는 무료 성인사이트 \n",
              "입니다.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: ## Search Results\n",
              "\n",
              "|최신 한국녀 포르노 영상들, #2 - xHamster](https://ko.xhamster2.com/categories/korean/newest/2)\n",
              "27 June 2025 오류 로딩 썸네일 08:12 후장 섹스로 오르가즘을 느끼는 한국 아시아 소녀 후장 RealDomSpank 3.9K views 18 \n",
              "June 2025\n",
              "\n",
              "|/todays-selection | 성인 야동 사이트 - 아라곤19 | aragon19.org](https://aragon19.org/)\n",
              "/todays-selection - 아라곤19 에서 제공하는 무료야동 및 성인 야동사이트로 일본야동 서양야동 동양야동 한국야동 \n",
              "동영상을 실시간 업데이트 합니다.\n",
              "\n",
              "|KBJFan - Watch Free Korean BJ Online](https://www.kbjfan.com/)\n",
              "KBJFan provides Korean BJ live stream and recorded videos. We update Korean BJ record videos every day. Enjoy Free \n",
              "Korean BJ shows online without ads!\n",
              "\n",
              "|야코주소 (YAKO) | 무료성인사이트, 한국야동](https://www.yako.gg/best?cate=korea)\n",
              "야코주소 (YAKO)는 실시간으로 한국야동과 일본야동, 최신야동, 성인야동을 무료로 시청하실 수 있는 무료 성인사이트 \n",
              "입니다.\n",
              "\n",
              "|국산 Porn Videos | Pornhub.com](https://www.pornhub.com/video/search?search=국산)\n",
              "국산 Porn Videos chinese hd sex sex machine 한국 야동 asian korean 한국 야동 국산 야동 onlyfans korean korean \n",
              "couple 한국 섹스 한국 bj 국산 야동 japanese 허누나 korean porn 한국 …\n",
              "\n",
              "|YouTube Korea - YouTube](https://www.youtube.com/@youtubekorea)\n",
              "YouTube Korea 공식 채널에서는 다양한 종류의 콘텐츠를 소개하고 있습니다. 채널 구독을 통해 국내외에서 화제가 되고 \n",
              "있는 최신 동영상을 확인해보세요.\n",
              "\n",
              "|무료 한국인 비제이 포르노 비디오 | xHamster](https://ko.xhamster2.com/tags/korean-bj)\n",
              "xHamster에서 한국인 비제이 포르노 비디오를 확인하세요. 지금 모든 한국인 비제이 XXX 비디오를 시청하세요!\n",
              "\n",
              "|한국녀 포르노 영상들, #3 - xHamster](https://ko.xhamster2.com/categories/korean/3)\n",
              "8.2M views 오류 로딩 썸네일 07:01 내 한국 의붓여동생의 보지가 너무 완벽해, 나는 그것을 쳐서 사정해 OniSoi 81.7K \n",
              "views 오류 로딩 썸네일\n",
              "\n",
              "|야코주소 (YAKO) | 무료성인사이트, 한국야동](https://www.yako.gg/all?page=2)\n",
              "야동코리아(YAKO)는 실시간으로 한국야동과 일본야동, 최신야동, 성인야동을 무료로 시청하실 수 있는 무료 성인사이트 \n",
              "입니다.\n",
              "\n",
              "|야코주소 (YAKO) | 무료성인사이트, 한국야동](https://www.yako.gg/best?cate=all)\n",
              "야코주소 (YAKO)는 실시간으로 한국야동과 일본야동, 최신야동, 성인야동을 무료로 시청하실 수 있는 무료 성인사이트 \n",
              "입니다.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Observations: ## Search Results\n",
              "\n",
              "|LibHunt flash-attention vs xformers - compare differences and reviews? | \n",
              "LibHunt](https://www.libhunt.com/compare-flash-attention-vs-xformers)\n",
              "Longer sequence length in transformers is an active area of research (see e.g the great work from the Flash - \n",
              "attention team - https://github.com/HazyResearch/ flash - attention ), and I'm sure will improve things \n",
              "dramatically very soon. Posts with mentions or reviews of xformers .\n",
              "\n",
              "|Pikoo Re-evaluating: The True Power of Flash Attention 2 - 𝓛𝓸𝓸𝓾𝓲𝓼' \n",
              "𝓑𝓵𝓸𝓰](https://pikoo.de/posts/flash_attention_2_is_very_good/)\n",
              "February 8, 2025 - Even with a bandwidth difference of over 60%, the acceleration from flash attention 2 can \n",
              "forcefully equalize the generation throughput. This means that the improvement over xformers is at least 40%, and \n",
              "in some scenarios, it can reach 70% . Hence, this article presents several conclusions without ...\n",
              "\n",
              "|Attention \n",
              "机制哪家强？SDPA、FlashAttention、xFormers、手动实现全...](https://zhuanlan.zhihu.com/p/1898470649938293363)\n",
              "不同的 Attention 实现方式（如 PyTorch 官方的 scaled_dot_product_attention （SDPA）、 Flash At tention 、 x Form ers\n",
              "和手动实现）在计算效率、显存占用、数值精度等方面表现如何？\n",
              "\n",
              "|GitHub Comparison with FlashAttention · Issue #950 · \n",
              "facebookresearch/xformers](https://github.com/facebookresearch/xformers/issues/950)\n",
              "December 20, 2023 - Have the xformers already supported Flash Attention (or include the algorithm in \n",
              "memory_efficient_attention)? When should I use xformers or flash attention? Flash attention can be easily applied \n",
              "by using monkey patch without modifying the original code while xformers is a bit complicated .\n",
              "\n",
              "|GitHub Flash Attention v2 achieved 44% faster than xformers/pytorch_sdp_attention on large image · \n",
              "AUTOMATIC1111/stable-diffusion-webui · Discussion \n",
              "#11884](https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/11884)\n",
              "The output image may has very little difference from baseline, just like xformers or pytorch_sdp_ attention . I \n",
              "think it's worth to been added to stable diffusion webui, and I will soon make a draft PR about this. However, \n",
              "their are still some problems to discuss: for now, install flash -attn version ...\n",
              "\n",
              "|Facebook Research xFormers optimized operators | xFormers 0.0.33 \n",
              "documentation](https://facebookresearch.github.io/xformers/components/ops.html)\n",
              "Operator that computes memory-efficient attention using Flash - Attention implementation. ... This file contains \n",
              "biases that can be used as the attn_bias argument in xformers .ops.memory_efficient_ attention .\n",
              "\n",
              "|Hugging Face Flash attention has no effect on inference - 🤗Transformers - Hugging Face \n",
              "Forums](https://discuss.huggingface.co/t/flash-attention-has-no-effect-on-inference/73453)\n",
              "February 16, 2024 - Hi, I was exploring the benefits of using flash attention 2 with Mistral and Mixtral during \n",
              "inference. Yet, I can see no memory reduction & no speed acceleration. Some number under different attention \n",
              "implementations: Mixtral (mistralai/Mixtral-8x7B-Instruct-v0.1): attn_implementation=‘f...\n",
              "\n",
              "|PyTorch FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision – \n",
              "PyTorch](https://pytorch.org/blog/flashattention-3/)\n",
              "Attention, as a core layer of the ubiquitous Transformer architecture, is a bottleneck for large language models \n",
              "and long-context applications. FlashAttention (and FlashAttention-2) pioneered an approach to speed up attention on\n",
              "GPUs by minimizing memory reads/writes , and is now used by most ...\n",
              "\n",
              "|arXiv |2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention with \n",
              "IO-Awareness](https://arxiv.org/abs/2205.14135)\n",
              "June 23, 2022 - Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of \n",
              "self- attention are quadratic in sequence length. Approximate attention methods have attempted to address this \n",
              "problem by trading off model quality to reduce the compute complexity, but often do ...\n",
              "\n",
              "|PyTorch Forums What are the differences between Flash Attention and Memory Efficient Attention? - PyTorch \n",
              "Forums](https://discuss.pytorch.org/t/what-are-the-differences-between-flash-attention-and-memory-efficient-attenti\n",
              "on/183561)\n",
              "July 6, 2023 - I’m learning about PyTorch and Transformer. While reading the source code of PyTorch, I noticed that\n",
              "if I don’t enable the USE_ FLASH _ ATTENTION compilation condition, the memory efficient attention won’t be \n",
              "compiled into PyTorch. Does this mean that the implementation of memory-efficient ...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: ## Search Results\n",
              "\n",
              "|LibHunt flash-attention vs xformers - compare differences and reviews? | \n",
              "LibHunt](https://www.libhunt.com/compare-flash-attention-vs-xformers)\n",
              "Longer sequence length in transformers is an active area of research (see e.g the great work from the Flash - \n",
              "attention team - https://github.com/HazyResearch/ flash - attention ), and I'm sure will improve things \n",
              "dramatically very soon. Posts with mentions or reviews of xformers .\n",
              "\n",
              "|Pikoo Re-evaluating: The True Power of Flash Attention 2 - 𝓛𝓸𝓸𝓾𝓲𝓼' \n",
              "𝓑𝓵𝓸𝓰](https://pikoo.de/posts/flash_attention_2_is_very_good/)\n",
              "February 8, 2025 - Even with a bandwidth difference of over 60%, the acceleration from flash attention 2 can \n",
              "forcefully equalize the generation throughput. This means that the improvement over xformers is at least 40%, and \n",
              "in some scenarios, it can reach 70% . Hence, this article presents several conclusions without ...\n",
              "\n",
              "|Attention \n",
              "机制哪家强？SDPA、FlashAttention、xFormers、手动实现全...](https://zhuanlan.zhihu.com/p/1898470649938293363)\n",
              "不同的 Attention 实现方式（如 PyTorch 官方的 scaled_dot_product_attention （SDPA）、 Flash At tention 、 x Form ers\n",
              "和手动实现）在计算效率、显存占用、数值精度等方面表现如何？\n",
              "\n",
              "|GitHub Comparison with FlashAttention · Issue #950 · \n",
              "facebookresearch/xformers](https://github.com/facebookresearch/xformers/issues/950)\n",
              "December 20, 2023 - Have the xformers already supported Flash Attention (or include the algorithm in \n",
              "memory_efficient_attention)? When should I use xformers or flash attention? Flash attention can be easily applied \n",
              "by using monkey patch without modifying the original code while xformers is a bit complicated .\n",
              "\n",
              "|GitHub Flash Attention v2 achieved 44% faster than xformers/pytorch_sdp_attention on large image · \n",
              "AUTOMATIC1111/stable-diffusion-webui · Discussion \n",
              "#11884](https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/11884)\n",
              "The output image may has very little difference from baseline, just like xformers or pytorch_sdp_ attention . I \n",
              "think it's worth to been added to stable diffusion webui, and I will soon make a draft PR about this. However, \n",
              "their are still some problems to discuss: for now, install flash -attn version ...\n",
              "\n",
              "|Facebook Research xFormers optimized operators | xFormers 0.0.33 \n",
              "documentation](https://facebookresearch.github.io/xformers/components/ops.html)\n",
              "Operator that computes memory-efficient attention using Flash - Attention implementation. ... This file contains \n",
              "biases that can be used as the attn_bias argument in xformers .ops.memory_efficient_ attention .\n",
              "\n",
              "|Hugging Face Flash attention has no effect on inference - 🤗Transformers - Hugging Face \n",
              "Forums](https://discuss.huggingface.co/t/flash-attention-has-no-effect-on-inference/73453)\n",
              "February 16, 2024 - Hi, I was exploring the benefits of using flash attention 2 with Mistral and Mixtral during \n",
              "inference. Yet, I can see no memory reduction &amp; no speed acceleration. Some number under different attention \n",
              "implementations: Mixtral (mistralai/Mixtral-8x7B-Instruct-v0.1): attn_implementation=‘f...\n",
              "\n",
              "|PyTorch FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision – \n",
              "PyTorch](https://pytorch.org/blog/flashattention-3/)\n",
              "Attention, as a core layer of the ubiquitous Transformer architecture, is a bottleneck for large language models \n",
              "and long-context applications. FlashAttention (and FlashAttention-2) pioneered an approach to speed up attention on\n",
              "GPUs by minimizing memory reads/writes , and is now used by most ...\n",
              "\n",
              "|arXiv |2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention with \n",
              "IO-Awareness](https://arxiv.org/abs/2205.14135)\n",
              "June 23, 2022 - Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of \n",
              "self- attention are quadratic in sequence length. Approximate attention methods have attempted to address this \n",
              "problem by trading off model quality to reduce the compute complexity, but often do ...\n",
              "\n",
              "|PyTorch Forums What are the differences between Flash Attention and Memory Efficient Attention? - PyTorch \n",
              "Forums](https://discuss.pytorch.org/t/what-are-the-differences-between-flash-attention-and-memory-efficient-attenti\n",
              "on/183561)\n",
              "July 6, 2023 - I’m learning about PyTorch and Transformer. While reading the source code of PyTorch, I noticed that\n",
              "if I don’t enable the USE_ FLASH _ ATTENTION compilation condition, the memory efficient attention won’t be \n",
              "compiled into PyTorch. Does this mean that the implementation of memory-efficient ...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Observations: ## Search Results\n",
              "\n",
              "|FlashAttention on a Napkin: A Diagrammatic Approach to Deep](https://arxiv.org/html/2412.03317v2)\n",
              "FlashAttention (Dao et al., 2022 ; Dao, 2023 ; Shah et al., 2024 ) is an IO-aware approach to attention that \n",
              "overcomes the memory wall.\n",
              "\n",
              "|SystolicAttention: Fusing FlashAttention within a Single](https://arxiv.org/html/2507.11331v1)\n",
              "In this section, we first provide an overview of the enhanced systolic array microarchitecture, FSA, that we \n",
              "propose to accelerate the FlashAttention ...\n",
              "\n",
              "|FlashAttention-3: Fast and Accurate Attention with Asynchrony](https://www.together.ai/blog/flashattention-3)\n",
              "While FlashAttention -2 can achieve up to 70% theoretical max FLOPS on Ampere (A100) GPUs, it does not yet take \n",
              "advantage of new features on Hopper ...\n",
              "\n",
              "|FlashAttention: Fast and memory-efficient exact attention with](https://www.together.ai/blog/flashattentionfandm)\n",
              "We propose FlashAttention , an IO-aware exact attention algorithm that uses tiling to reduce the number of memory \n",
              "reads/writes between GPU high ...\n",
              "\n",
              "|Paper Summary #8 - FlashAttention: Fast and \n",
              "Memory-Efficient](https://shreyansh26.github.io/post/2023-03-26_flash-attention/)\n",
              "Since FlashAttention computes exact ... As mentioned in the overview , FlashAttention can be used to make a \n",
              "approximate attention algorithm as well.\n",
              "\n",
              "|GPU inference](https://huggingface.co/docs/transformers/v4.41.2/perf_infer_gpu_one)\n",
              "FlashAttention is more memory efficient, meaning you can train on much larger sequence lengths without running into\n",
              "out-of-memory issues.\n",
              "\n",
              "|sbarman25 (Snehasish Barman)](https://huggingface.co/sbarman25)\n",
              "FlashAttention -2: Faster Attention with Better Parallelism and Work Partitioning ... Understanding LLMs: A \n",
              "Comprehensive Overview from Training to ...\n",
              "\n",
              "|Aman's AI Journal • Primers • Model Acceleration](https://aman.ai/primers/ai/model-acceleration/)\n",
              "FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality \n",
              "models (0.7 better perplexity on GPT-2 ...\n",
              "\n",
              "|Tag: Transformer | Chenfan Blog](https://jcf94.com/tags/Transformer/)\n",
              "To live is to change the world. ... FlashAttentions ... Overview\n",
              "\n",
              "|Recall to ALU/FPU 的硬件实现 | Chenfan Blog](https://jcf94.com/2024/09/08/2024-09-08-alu-fpu/)\n",
              "... FlashAttentions ... Overview\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: ## Search Results\n",
              "\n",
              "|FlashAttention on a Napkin: A Diagrammatic Approach to Deep](https://arxiv.org/html/2412.03317v2)\n",
              "FlashAttention (Dao et al., 2022 ; Dao, 2023 ; Shah et al., 2024 ) is an IO-aware approach to attention that \n",
              "overcomes the memory wall.\n",
              "\n",
              "|SystolicAttention: Fusing FlashAttention within a Single](https://arxiv.org/html/2507.11331v1)\n",
              "In this section, we first provide an overview of the enhanced systolic array microarchitecture, FSA, that we \n",
              "propose to accelerate the FlashAttention ...\n",
              "\n",
              "|FlashAttention-3: Fast and Accurate Attention with Asynchrony](https://www.together.ai/blog/flashattention-3)\n",
              "While FlashAttention -2 can achieve up to 70% theoretical max FLOPS on Ampere (A100) GPUs, it does not yet take \n",
              "advantage of new features on Hopper ...\n",
              "\n",
              "|FlashAttention: Fast and memory-efficient exact attention with](https://www.together.ai/blog/flashattentionfandm)\n",
              "We propose FlashAttention , an IO-aware exact attention algorithm that uses tiling to reduce the number of memory \n",
              "reads/writes between GPU high ...\n",
              "\n",
              "|Paper Summary #8 - FlashAttention: Fast and \n",
              "Memory-Efficient](https://shreyansh26.github.io/post/2023-03-26_flash-attention/)\n",
              "Since FlashAttention computes exact ... As mentioned in the overview , FlashAttention can be used to make a \n",
              "approximate attention algorithm as well.\n",
              "\n",
              "|GPU inference](https://huggingface.co/docs/transformers/v4.41.2/perf_infer_gpu_one)\n",
              "FlashAttention is more memory efficient, meaning you can train on much larger sequence lengths without running into\n",
              "out-of-memory issues.\n",
              "\n",
              "|sbarman25 (Snehasish Barman)](https://huggingface.co/sbarman25)\n",
              "FlashAttention -2: Faster Attention with Better Parallelism and Work Partitioning ... Understanding LLMs: A \n",
              "Comprehensive Overview from Training to ...\n",
              "\n",
              "|Aman's AI Journal • Primers • Model Acceleration](https://aman.ai/primers/ai/model-acceleration/)\n",
              "FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality \n",
              "models (0.7 better perplexity on GPT-2 ...\n",
              "\n",
              "|Tag: Transformer | Chenfan Blog](https://jcf94.com/tags/Transformer/)\n",
              "To live is to change the world. ... FlashAttentions ... Overview\n",
              "\n",
              "|Recall to ALU/FPU 的硬件实现 | Chenfan Blog](https://jcf94.com/2024/09/08/2024-09-08-alu-fpu/)\n",
              "... FlashAttentions ... Overview\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 4.88 seconds| Input tokens: 1,125 | Output tokens: 71]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 4.88 seconds| Input tokens: 1,125 | Output tokens: 71]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ Calling tool: 'visit_webpage' with arguments: {'url':                                                           │\n",
              "│ 'https://www.libhunt.com/compare-flash-attention-vs-xformers'}                                                  │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ Calling tool: 'visit_webpage' with arguments: {'url':                                                           │\n",
              "│ 'https://www.libhunt.com/compare-flash-attention-vs-xformers'}                                                  │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ Calling tool: 'visit_webpage' with arguments: {'url': 'https://pikoo.de/posts/flash_attention_2_is_very_good/'} │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ Calling tool: 'visit_webpage' with arguments: {'url': 'https://pikoo.de/posts/flash_attention_2_is_very_good/'} │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ Calling tool: 'visit_webpage' with arguments: {'url': 'https://arxiv.org/abs/2205.14135'}                       │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ Calling tool: 'visit_webpage' with arguments: {'url': 'https://arxiv.org/abs/2205.14135'}                       │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Observations: |2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n",
              "\n",
              "  \n",
              "\n",
              "|Skip to main content](#content)\n",
              "\n",
              "|!|Cornell \n",
              "University](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n",
              "\n",
              "We gratefully acknowledge support from the Simons Foundation, |member \n",
              "institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.\n",
              "|Donate](https://info.arxiv.org/about/donate.html)\n",
              "\n",
              "|!|arxiv logo](/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](/) > |cs](/list/cs/recent) > \n",
              "arXiv:2205.14135\n",
              "\n",
              "|Help](https://info.arxiv.org/help) | |Advanced Search](https://arxiv.org/search/advanced)\n",
              "\n",
              "All fields\n",
              "Title\n",
              "Author\n",
              "Abstract\n",
              "Comments\n",
              "Journal reference\n",
              "ACM classification\n",
              "MSC classification\n",
              "Report number\n",
              "arXiv identifier\n",
              "DOI\n",
              "ORCID\n",
              "arXiv author ID\n",
              "Help pages\n",
              "Full text\n",
              "\n",
              "Search\n",
              "\n",
              "|!|arXiv logo](/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n",
              "\n",
              "|!|Cornell University \n",
              "Logo](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n",
              "\n",
              "open search\n",
              "\n",
              "GO\n",
              "\n",
              "open navigation menu\n",
              "\n",
              "quick links\n",
              "-----------\n",
              "\n",
              "* |Login](https://arxiv.org/login)\n",
              "* |Help Pages](https://info.arxiv.org/help)\n",
              "* |About](https://info.arxiv.org/about)\n",
              "\n",
              "Computer Science > Machine Learning\n",
              "===================================\n",
              "\n",
              "**arXiv:2205.14135** (cs)\n",
              "\n",
              "|Submitted on 27 May 2022 (|v1](https://arxiv.org/abs/2205.14135v1)), last revised 23 Jun 2022 (this version, v2)]\n",
              "\n",
              "Title:FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n",
              "=================================================================================\n",
              "\n",
              "Authors:|Tri Dao](https://arxiv.org/search/cs?searchtype=author&query=Dao,+T), |Daniel Y. \n",
              "Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu,+D+Y), |Stefano \n",
              "Ermon](https://arxiv.org/search/cs?searchtype=author&query=Ermon,+S), |Atri \n",
              "Rudra](https://arxiv.org/search/cs?searchtype=author&query=Rudra,+A), |Christopher \n",
              "Ré](https://arxiv.org/search/cs?searchtype=author&query=R%C3%A9,+C)\n",
              "\n",
              "View a PDF of the paper titled FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, by Tri \n",
              "Dao and 4 other authors\n",
              "\n",
              "|View PDF](/pdf/2205.14135)\n",
              "> Abstract:Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of \n",
              "self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this \n",
              "problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup.\n",
              "We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes \n",
              "between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to \n",
              "reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze \n",
              "the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is \n",
              "optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate\n",
              "attention algorithm that is faster than any existing approximate attention method. FlashAttention trains \n",
              "Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) \n",
              "compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ \n",
              "speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer \n",
              "context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on \n",
              "long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance \n",
              "performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% \n",
              "accuracy).\n",
              "\n",
              "|  |  |\n",
              "| --- | --- |\n",
              "| Subjects: | Machine Learning (cs.LG) |\n",
              "| Cite as: | |arXiv:2205.14135](https://arxiv.org/abs/2205.14135) |cs.LG] |\n",
              "|  | (or  |arXiv:2205.14135v2](https://arxiv.org/abs/2205.14135v2) |cs.LG] for this version) |\n",
              "|  | <https://doi.org/10.48550/arXiv.2205.14135> Focus to learn more  arXiv-issued DOI via DataCite |\n",
              "\n",
              "Submission history\n",
              "------------------\n",
              "\n",
              "From: Tri Dao ||view email](/show-email/00d4bef1/2205.14135)]   \n",
              " **||v1]](/abs/2205.14135v1)**\n",
              "Fri, 27 May 2022 17:53:09 UTC (1,325 KB)  \n",
              "**|v2]**\n",
              "Thu, 23 Jun 2022 17:53:32 UTC (1,653 KB)\n",
              "\n",
              "Full-text links:\n",
              "\n",
              "Access Paper:\n",
              "-------------\n",
              "\n",
              "View a PDF of the paper titled FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, by Tri \n",
              "Dao and 4 other authors\n",
              "\n",
              "* |View PDF](/pdf/2205.14135)\n",
              "* |TeX Source](/src/2205.14135)\n",
              "* |Other Formats](/format/2205.14135)\n",
              "\n",
              "|view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \"Rights to this article\")\n",
              "\n",
              "Current browse context:\n",
              "\n",
              "cs.LG\n",
              "\n",
              "|< prev](/prevnext?id=2205.14135&function=prev&context=cs.LG \"previous in cs.LG (accesskey p)\")\n",
              "  |   \n",
              "|next >](/prevnext?id=2205.14135&function=next&context=cs.LG \"next in cs.LG (accesskey n)\")\n",
              "\n",
              "|new](/list/cs.LG/new)\n",
              " | \n",
              "|recent](/list/cs.LG/recent)\n",
              " | |2022-05](/list/cs.LG/2022-05)\n",
              "\n",
              "Change to browse by:\n",
              "\n",
              "|cs](/abs/2205.14135?context=cs)\n",
              "\n",
              "### References & Citations\n",
              "\n",
              "* |NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2205.14135)\n",
              "* |Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2205.14135)\n",
              "* |Semantic Scholar](https://api.semanticscholar.org/arXiv:2205.14135)\n",
              "\n",
              "### |1 blog link](/tb/2205.14135)\n",
              "\n",
              "(|what is this?](https://info.arxiv.org/help/trackback.html))\n",
              "\n",
              "|a](/static/browse/0.3.4/css/cite.css)\n",
              "export BibTeX citation\n",
              "Loading...\n",
              "\n",
              "BibTeX formatted citation\n",
              "-------------------------\n",
              "\n",
              "×\n",
              "\n",
              "loading...\n",
              "\n",
              "Data provided by:\n",
              "\n",
              "### Bookmark\n",
              "\n",
              "|!|BibSonomy \n",
              "logo](/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=uplo\n",
              "ad&url=https://arxiv.org/abs/2205.14135&description=FlashAttention: Fast and Memory-Efficient Exact Attention with \n",
              "IO-Awareness \"Bookmark on BibSonomy\")\n",
              "|!|Reddit \n",
              "logo](/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/220\n",
              "5.14135&title=FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness \"Bookmark on Reddit\")\n",
              "\n",
              "Bibliographic Tools\n",
              "\n",
              "Bibliographic and Citation Tools\n",
              "================================\n",
              "\n",
              "Bibliographic Explorer Toggle\n",
              "\n",
              "Bibliographic Explorer *(|What is the \n",
              "Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\n",
              "\n",
              "Connected Papers Toggle\n",
              "\n",
              "Connected Papers *(|What is Connected Papers?](https://www.connectedpapers.com/about))*\n",
              "\n",
              "Litmaps Toggle\n",
              "\n",
              "Litmaps *(|What is Litmaps?](https://www.litmaps.co/))*\n",
              "\n",
              "scite.ai Toggle\n",
              "\n",
              "scite Smart Citations *(|What are Smart Citations?](https://www.scite.ai/))*\n",
              "\n",
              "Code, Data, Media\n",
              "\n",
              "Code, Data and Media Associated with this Article\n",
              "=================================================\n",
              "\n",
              "alphaXiv Toggle\n",
              "\n",
              "alphaXiv *(|What is alphaXiv?](https://alphaxiv.org/))*\n",
              "\n",
              "Links to Code Toggle\n",
              "\n",
              "CatalyzeX Code Finder for Papers *(|What is CatalyzeX?](https://www.catalyzex.com))*\n",
              "\n",
              "DagsHub Toggle\n",
              "\n",
              "DagsHub *(|What is DagsHub?](https://dagshub.com/))*\n",
              "\n",
              "GotitPub Toggle\n",
              "\n",
              "Gotit.pub *(|What is GotitPub?](http://gotit.pub/faq))*\n",
              "\n",
              "Huggingface Toggle\n",
              "\n",
              "Hugging Face *(|What is Huggingface?](https://huggingface.co/huggingface))*\n",
              "\n",
              "Links to Code Toggle\n",
              "\n",
              "Papers with Code *(|What is Papers with Code?](https://paperswithcode.com/))*\n",
              "\n",
              "ScienceCast Toggle\n",
              "\n",
              "ScienceCast *(|What is ScienceCast?](https://sciencecast.org/welcome))*\n",
              "\n",
              "Demos\n",
              "\n",
              "Demos\n",
              "=====\n",
              "\n",
              "Replicate Toggle\n",
              "\n",
              "Replicate *(|What is Replicate?](https://replicate.com/docs/arxiv/about))*\n",
              "\n",
              "Spaces Toggle\n",
              "\n",
              "Hugging Face Spaces *(|What is Spaces?](https://huggingface.co/docs/hub/spaces))*\n",
              "\n",
              "Spaces Toggle\n",
              "\n",
              "TXYZ.AI *(|What is TXYZ.AI?](https://txyz.ai))*\n",
              "\n",
              "Related Papers\n",
              "\n",
              "Recommenders and Search Tools\n",
              "=============================\n",
              "\n",
              "Link to Influence Flower\n",
              "\n",
              "Influence Flower *(|What are Influence Flowers?](https://influencemap.cmlab.dev/))*\n",
              "\n",
              "Core recommender toggle\n",
              "\n",
              "CORE Recommender *(|What is CORE?](https://core.ac.uk/services/recommender))*\n",
              "\n",
              "IArxiv recommender toggle\n",
              "\n",
              "IArxiv Recommender\n",
              "*(|What is IArxiv?](https://iarxiv.org/about))*\n",
              "\n",
              "* Author\n",
              "* Venue\n",
              "* Institution\n",
              "* Topic\n",
              "\n",
              "About arXivLabs\n",
              "\n",
              "arXivLabs: experimental projects with community collaborators\n",
              "=============================================================\n",
              "\n",
              "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n",
              "\n",
              "Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, \n",
              "community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that \n",
              "adhere to them.\n",
              "\n",
              "Have an idea for a project that will add value for arXiv's community? |**Learn more about \n",
              "arXivLabs**](https://info.arxiv.org/labs/index.html).\n",
              "\n",
              "|Which authors of this paper are endorsers?](/auth/show-endorsers/2205.14135) |\n",
              "|Disable MathJax](javascript:setMathjaxCookie()) (|What is MathJax?](https://info.arxiv.org/help/mathjax.html))\n",
              "\n",
              "* |About](https://info.arxiv.org/about)\n",
              "* |Help](https://info.arxiv.org/help)\n",
              "\n",
              "* contact arXivClick here to contact arXiv\n",
              "   |Contact](https://info.arxiv.org/help/contact.html)\n",
              "* subscribe to arXiv mailingsClick here to subscribe\n",
              "   |Subscribe](https://info.arxiv.org/help/subscribe)\n",
              "\n",
              "* |Copyright](https://info.arxiv.org/help/license/index.html)\n",
              "* |Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n",
              "\n",
              "* |Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n",
              "* |arXiv Operational Status](https://status.arxiv.org)   \n",
              "  Get status notifications via\n",
              "  |email](https://subscribe.sorryapp.com/24846f03/email/new)\n",
              "  or |slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: |2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n",
              "\n",
              "  \n",
              "\n",
              "|Skip to main content](#content)\n",
              "\n",
              "|!|Cornell \n",
              "University](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n",
              "\n",
              "We gratefully acknowledge support from the Simons Foundation, |member \n",
              "institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.\n",
              "|Donate](https://info.arxiv.org/about/donate.html)\n",
              "\n",
              "|!|arxiv logo](/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](/) &gt; |cs](/list/cs/recent) &gt; \n",
              "arXiv:2205.14135\n",
              "\n",
              "|Help](https://info.arxiv.org/help) | |Advanced Search](https://arxiv.org/search/advanced)\n",
              "\n",
              "All fields\n",
              "Title\n",
              "Author\n",
              "Abstract\n",
              "Comments\n",
              "Journal reference\n",
              "ACM classification\n",
              "MSC classification\n",
              "Report number\n",
              "arXiv identifier\n",
              "DOI\n",
              "ORCID\n",
              "arXiv author ID\n",
              "Help pages\n",
              "Full text\n",
              "\n",
              "Search\n",
              "\n",
              "|!|arXiv logo](/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n",
              "\n",
              "|!|Cornell University \n",
              "Logo](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n",
              "\n",
              "open search\n",
              "\n",
              "GO\n",
              "\n",
              "open navigation menu\n",
              "\n",
              "quick links\n",
              "-----------\n",
              "\n",
              "* |Login](https://arxiv.org/login)\n",
              "* |Help Pages](https://info.arxiv.org/help)\n",
              "* |About](https://info.arxiv.org/about)\n",
              "\n",
              "Computer Science &gt; Machine Learning\n",
              "===================================\n",
              "\n",
              "**arXiv:2205.14135** (cs)\n",
              "\n",
              "|Submitted on 27 May 2022 (|v1](https://arxiv.org/abs/2205.14135v1)), last revised 23 Jun 2022 (this version, v2)]\n",
              "\n",
              "Title:FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n",
              "=================================================================================\n",
              "\n",
              "Authors:|Tri Dao](https://arxiv.org/search/cs?searchtype=author&amp;query=Dao,+T), |Daniel Y. \n",
              "Fu](https://arxiv.org/search/cs?searchtype=author&amp;query=Fu,+D+Y), |Stefano \n",
              "Ermon](https://arxiv.org/search/cs?searchtype=author&amp;query=Ermon,+S), |Atri \n",
              "Rudra](https://arxiv.org/search/cs?searchtype=author&amp;query=Rudra,+A), |Christopher \n",
              "Ré](https://arxiv.org/search/cs?searchtype=author&amp;query=R%C3%A9,+C)\n",
              "\n",
              "View a PDF of the paper titled FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, by Tri \n",
              "Dao and 4 other authors\n",
              "\n",
              "|View PDF](/pdf/2205.14135)\n",
              "&gt; Abstract:Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of \n",
              "self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this \n",
              "problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup.\n",
              "We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes \n",
              "between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to \n",
              "reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze \n",
              "the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is \n",
              "optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate\n",
              "attention algorithm that is faster than any existing approximate attention method. FlashAttention trains \n",
              "Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) \n",
              "compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ \n",
              "speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer \n",
              "context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on \n",
              "long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance \n",
              "performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% \n",
              "accuracy).\n",
              "\n",
              "|  |  |\n",
              "| --- | --- |\n",
              "| Subjects: | Machine Learning (cs.LG) |\n",
              "| Cite as: | |arXiv:2205.14135](https://arxiv.org/abs/2205.14135) |cs.LG] |\n",
              "|  | (or  |arXiv:2205.14135v2](https://arxiv.org/abs/2205.14135v2) |cs.LG] for this version) |\n",
              "|  | &lt;https://doi.org/10.48550/arXiv.2205.14135&gt; Focus to learn more  arXiv-issued DOI via DataCite |\n",
              "\n",
              "Submission history\n",
              "------------------\n",
              "\n",
              "From: Tri Dao ||view email](/show-email/00d4bef1/2205.14135)]   \n",
              " **||v1]](/abs/2205.14135v1)**\n",
              "Fri, 27 May 2022 17:53:09 UTC (1,325 KB)  \n",
              "**|v2]**\n",
              "Thu, 23 Jun 2022 17:53:32 UTC (1,653 KB)\n",
              "\n",
              "Full-text links:\n",
              "\n",
              "Access Paper:\n",
              "-------------\n",
              "\n",
              "View a PDF of the paper titled FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, by Tri \n",
              "Dao and 4 other authors\n",
              "\n",
              "* |View PDF](/pdf/2205.14135)\n",
              "* |TeX Source](/src/2205.14135)\n",
              "* |Other Formats](/format/2205.14135)\n",
              "\n",
              "|view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \"Rights to this article\")\n",
              "\n",
              "Current browse context:\n",
              "\n",
              "cs.LG\n",
              "\n",
              "|&lt; prev](/prevnext?id=2205.14135&amp;function=prev&amp;context=cs.LG \"previous in cs.LG (accesskey p)\")\n",
              "  |   \n",
              "|next &gt;](/prevnext?id=2205.14135&amp;function=next&amp;context=cs.LG \"next in cs.LG (accesskey n)\")\n",
              "\n",
              "|new](/list/cs.LG/new)\n",
              " | \n",
              "|recent](/list/cs.LG/recent)\n",
              " | |2022-05](/list/cs.LG/2022-05)\n",
              "\n",
              "Change to browse by:\n",
              "\n",
              "|cs](/abs/2205.14135?context=cs)\n",
              "\n",
              "### References &amp; Citations\n",
              "\n",
              "* |NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2205.14135)\n",
              "* |Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2205.14135)\n",
              "* |Semantic Scholar](https://api.semanticscholar.org/arXiv:2205.14135)\n",
              "\n",
              "### |1 blog link](/tb/2205.14135)\n",
              "\n",
              "(|what is this?](https://info.arxiv.org/help/trackback.html))\n",
              "\n",
              "|a](/static/browse/0.3.4/css/cite.css)\n",
              "export BibTeX citation\n",
              "Loading...\n",
              "\n",
              "BibTeX formatted citation\n",
              "-------------------------\n",
              "\n",
              "×\n",
              "\n",
              "loading...\n",
              "\n",
              "Data provided by:\n",
              "\n",
              "### Bookmark\n",
              "\n",
              "|!|BibSonomy \n",
              "logo](/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=uplo\n",
              "ad&amp;url=https://arxiv.org/abs/2205.14135&amp;description=FlashAttention: Fast and Memory-Efficient Exact Attention with \n",
              "IO-Awareness \"Bookmark on BibSonomy\")\n",
              "|!|Reddit \n",
              "logo](/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/220\n",
              "5.14135&amp;title=FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness \"Bookmark on Reddit\")\n",
              "\n",
              "Bibliographic Tools\n",
              "\n",
              "Bibliographic and Citation Tools\n",
              "================================\n",
              "\n",
              "Bibliographic Explorer Toggle\n",
              "\n",
              "Bibliographic Explorer *(|What is the \n",
              "Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\n",
              "\n",
              "Connected Papers Toggle\n",
              "\n",
              "Connected Papers *(|What is Connected Papers?](https://www.connectedpapers.com/about))*\n",
              "\n",
              "Litmaps Toggle\n",
              "\n",
              "Litmaps *(|What is Litmaps?](https://www.litmaps.co/))*\n",
              "\n",
              "scite.ai Toggle\n",
              "\n",
              "scite Smart Citations *(|What are Smart Citations?](https://www.scite.ai/))*\n",
              "\n",
              "Code, Data, Media\n",
              "\n",
              "Code, Data and Media Associated with this Article\n",
              "=================================================\n",
              "\n",
              "alphaXiv Toggle\n",
              "\n",
              "alphaXiv *(|What is alphaXiv?](https://alphaxiv.org/))*\n",
              "\n",
              "Links to Code Toggle\n",
              "\n",
              "CatalyzeX Code Finder for Papers *(|What is CatalyzeX?](https://www.catalyzex.com))*\n",
              "\n",
              "DagsHub Toggle\n",
              "\n",
              "DagsHub *(|What is DagsHub?](https://dagshub.com/))*\n",
              "\n",
              "GotitPub Toggle\n",
              "\n",
              "Gotit.pub *(|What is GotitPub?](http://gotit.pub/faq))*\n",
              "\n",
              "Huggingface Toggle\n",
              "\n",
              "Hugging Face *(|What is Huggingface?](https://huggingface.co/huggingface))*\n",
              "\n",
              "Links to Code Toggle\n",
              "\n",
              "Papers with Code *(|What is Papers with Code?](https://paperswithcode.com/))*\n",
              "\n",
              "ScienceCast Toggle\n",
              "\n",
              "ScienceCast *(|What is ScienceCast?](https://sciencecast.org/welcome))*\n",
              "\n",
              "Demos\n",
              "\n",
              "Demos\n",
              "=====\n",
              "\n",
              "Replicate Toggle\n",
              "\n",
              "Replicate *(|What is Replicate?](https://replicate.com/docs/arxiv/about))*\n",
              "\n",
              "Spaces Toggle\n",
              "\n",
              "Hugging Face Spaces *(|What is Spaces?](https://huggingface.co/docs/hub/spaces))*\n",
              "\n",
              "Spaces Toggle\n",
              "\n",
              "TXYZ.AI *(|What is TXYZ.AI?](https://txyz.ai))*\n",
              "\n",
              "Related Papers\n",
              "\n",
              "Recommenders and Search Tools\n",
              "=============================\n",
              "\n",
              "Link to Influence Flower\n",
              "\n",
              "Influence Flower *(|What are Influence Flowers?](https://influencemap.cmlab.dev/))*\n",
              "\n",
              "Core recommender toggle\n",
              "\n",
              "CORE Recommender *(|What is CORE?](https://core.ac.uk/services/recommender))*\n",
              "\n",
              "IArxiv recommender toggle\n",
              "\n",
              "IArxiv Recommender\n",
              "*(|What is IArxiv?](https://iarxiv.org/about))*\n",
              "\n",
              "* Author\n",
              "* Venue\n",
              "* Institution\n",
              "* Topic\n",
              "\n",
              "About arXivLabs\n",
              "\n",
              "arXivLabs: experimental projects with community collaborators\n",
              "=============================================================\n",
              "\n",
              "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n",
              "\n",
              "Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, \n",
              "community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that \n",
              "adhere to them.\n",
              "\n",
              "Have an idea for a project that will add value for arXiv's community? |**Learn more about \n",
              "arXivLabs**](https://info.arxiv.org/labs/index.html).\n",
              "\n",
              "|Which authors of this paper are endorsers?](/auth/show-endorsers/2205.14135) |\n",
              "|Disable MathJax](javascript:setMathjaxCookie()) (|What is MathJax?](https://info.arxiv.org/help/mathjax.html))\n",
              "\n",
              "* |About](https://info.arxiv.org/about)\n",
              "* |Help](https://info.arxiv.org/help)\n",
              "\n",
              "* contact arXivClick here to contact arXiv\n",
              "   |Contact](https://info.arxiv.org/help/contact.html)\n",
              "* subscribe to arXiv mailingsClick here to subscribe\n",
              "   |Subscribe](https://info.arxiv.org/help/subscribe)\n",
              "\n",
              "* |Copyright](https://info.arxiv.org/help/license/index.html)\n",
              "* |Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n",
              "\n",
              "* |Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n",
              "* |arXiv Operational Status](https://status.arxiv.org)   \n",
              "  Get status notifications via\n",
              "  |email](https://subscribe.sorryapp.com/24846f03/email/new)\n",
              "  or |slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Observations: Re-evaluating: The True Power of Flash Attention 2 - 𝓛𝓸𝓸𝓾𝓲𝓼' 𝓑𝓵𝓸𝓰\n",
              "\n",
              "|𝓛𝓸𝓸𝓾𝓲𝓼](/ \"𝓛𝓸𝓸𝓾𝓲𝓼' 𝓑𝓵𝓸𝓰\")\n",
              "\n",
              "|Posts](/posts/) |Tags](/tags/) |Categories](/categories/) \n",
              "\n",
              "|English简体中文](javascript:void(0); \"Select Language\")\n",
              "\n",
              "|𝓛𝓸𝓸𝓾𝓲𝓼](/ \"𝓛𝓸𝓸𝓾𝓲𝓼' 𝓑𝓵𝓸𝓰\")\n",
              "\n",
              "|Cancel](javascript:void(0);)\n",
              "\n",
              "|Posts](/posts/)|Tags](/tags/)|Categories](/categories/)|English简体中文](javascript:void(0); \"Select Language\")\n",
              "\n",
              "Contents\n",
              "--------\n",
              "\n",
              "Re-evaluating: The True Power of Flash Attention 2\n",
              "==================================================\n",
              "\n",
              "|Loouis](/ \"Author\") included in |AI](/categories/ai/)\n",
              "\n",
              "2025-02-08  1404 words \n",
              " 7 minutes\n",
              "\n",
              "!|/images/wallhaven-01887.png](/svg/loading.min.svg \"/images/wallhaven-01887.png\")\n",
              "\n",
              "Contents\n",
              "\n",
              "* |Test Results](#test-results)\n",
              "  + |AWQ Test Results](#awq-test-results)\n",
              "  + |FP16 Test Results](#fp16-test-results)\n",
              "\n",
              "> This article compares the performance differences of vllm when using xformers and flash attention 2 as the \n",
              "backend attention mechanism.\n",
              "\n",
              "Previously, I wrote an |article](https://pikoo.de/posts/flash_attention_2_underwhelming/) comparing the performance\n",
              "differences of vllm when using xformers and flash attention 2 as backend attention mechanisms. However, after a few\n",
              "days of reconsideration, I felt something was off. So, I conducted several more tests and discovered that flash \n",
              "attention 2 does indeed provide a significant boost. Even with a bandwidth difference of over 60%, the acceleration\n",
              "from flash attention 2 can forcefully equalize the generation throughput. This means that the improvement over \n",
              "xformers is at least 40%, and in some scenarios, it can reach 70%. Hence, this article presents several conclusions\n",
              "without extensive analysis:\n",
              "\n",
              "1. For the 30 series, changing the vllm environment variable `VLLM_ATTENTION_BACKEND=XFORMERS` might not actually \n",
              "use xformers, as the speed is consistent with fa2. However, there is a significant improvement compared to the \n",
              "2080ti.\n",
              "2. Using the lmdeploy turbomind inference engine on the 2080ti can achieve the same performance boost as fa2.\n",
              "3. The lmdeploy results on the 2080ti differ from previous tests, showing significant improvements in both awq and \n",
              "fp16.\n",
              "4. However, the 3070 shows different behavior compared to the above, so the 30 series (and presumably the 40 and 50\n",
              "series) should use vllm, as vllm seems to have specific optimizations for the 30 series and later.\n",
              "5. Depending on parameter ratios, fa2 provides an improvement of approximately 40-70% (for fp16 and awq \n",
              "quantization, respectively).\n",
              "\n",
              "Actual hardware parameters and single-request generation throughput extracted from the test results below:\n",
              "\n",
              "|  | tflops | bandwidth | vllm | lmdeploy | vllm | lmdeploy |\n",
              "| --- | --- | --- | --- | --- | --- | --- |\n",
              "| 3070laptop | 29.7 | 319 | 100 | 102 | 191 | 154 |\n",
              "| 2080ti | 52.5 | 526 | 97 | 176 | 228 | 305 |\n",
              "| ratio |  | 1.65 | 0.97 | 1.73 | 1.19 | 1.98 |\n",
              "\n",
              "Test Results\n",
              "------------\n",
              "\n",
              "### AWQ Test Results\n",
              "\n",
              "#### 3070 laptop\n",
              "\n",
              "##### vllm xformer\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 2.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 100.00 | 951.17 | 0.05 | 0.05 |\n",
              "| 2 | 196.12 | 885.67 | 0.10 | 0.10 |\n",
              "| 4 | 378.49 | 1568.36 | 0.10 | 0.12 |\n",
              "| 8 | 714.94 | 2434.35 | 0.13 | 0.15 |\n",
              "| 16 | 1211.09 | 2184.59 | 0.17 | 0.33 |\n",
              "| 32 | 1682.82 | 3098.94 | 0.14 | 0.47 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5638\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 2.20 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 72.75 | 3724.02 | 1.52 | 1.52 |\n",
              "| 2 | 65.69 | 3461.59 | 2.03 | 3.24 |\n",
              "| 4 | 118.01 | 3355.38 | 2.01 | 6.72 |\n",
              "| 8 | 129.15 | 3542.59 | 1.94 | 12.66 |\n",
              "\n",
              "##### vllm fa2\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 2.40 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 101.52 | 1106.82 | 0.04 | 0.04 |\n",
              "| 2 | 199.29 | 1326.93 | 0.07 | 0.07 |\n",
              "| 4 | 397.01 | 2322.60 | 0.08 | 0.08 |\n",
              "| 8 | 744.77 | 2998.94 | 0.04 | 0.12 |\n",
              "| 16 | 1280.79 | 3332.02 | 0.08 | 0.22 |\n",
              "| 32 | 1880.32 | 3579.14 | 0.05 | 0.40 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5519\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 3.20 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 65.94 | 2966.57 | 1.89 | 1.89 |\n",
              "| 2 | 113.30 | 3612.69 | 1.85 | 3.11 |\n",
              "| 4 | 58.97 | 3635.87 | 1.85 | 6.16 |\n",
              "| 8 | 53.07 | 3576.62 | 1.87 | 12.53 |\n",
              "\n",
              "##### lmdeploy turbomind\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 3.20 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 102.33 | 1162.42 | 0.04 | 0.04 |\n",
              "| 2 | 202.70 | 1441.08 | 0.03 | 0.07 |\n",
              "| 4 | 417.91 | 1724.37 | 0.03 | 0.11 |\n",
              "| 8 | 793.40 | 2305.94 | 0.03 | 0.16 |\n",
              "| 16 | 1365.04 | 3255.68 | 0.03 | 0.22 |\n",
              "| 32 | 1939.21 | 3398.90 | 0.03 | 0.43 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5622\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 2.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 26.89 | 4419.59 | 1.29 | 1.29 |\n",
              "| 2 | 129.84 | 4403.95 | 1.29 | 2.52 |\n",
              "| 4 | 145.21 | 4466.76 | 1.30 | 5.02 |\n",
              "| 8 | 172.47 | 4586.10 | 1.33 | 9.78 |\n",
              "\n",
              "#### 2080ti\n",
              "\n",
              "##### vllm xformer\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 1.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 97.29 | 716.20 | 0.06 | 0.06 |\n",
              "| 2 | 189.25 | 787.34 | 0.11 | 0.12 |\n",
              "| 4 | 368.60 | 1630.50 | 0.11 | 0.11 |\n",
              "| 8 | 673.36 | 2310.67 | 0.11 | 0.16 |\n",
              "| 16 | 1132.99 | 2949.96 | 0.12 | 0.25 |\n",
              "| 32 | 1561.65 | 4154.17 | 0.12 | 0.35 |\n",
              "| 64 | 1653.97 | 4995.70 | 0.14 | 0.58 |\n",
              "| 128 | 1795.95 | 5930.27 | 0.13 | 0.97 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5648\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 1.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 64.25 | 6055.06 | 0.93 | 0.93 |\n",
              "| 2 | 49.60 | 6011.82 | 0.99 | 1.84 |\n",
              "| 4 | 187.93 | 6402.48 | 1.48 | 3.52 |\n",
              "| 8 | 159.36 | 6402.51 | 5.11 | 7.04 |\n",
              "| 16 | 178.05 | 6434.66 | 4.73 | 13.92 |\n",
              "\n",
              "##### lmdeploy turbomind\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 1.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 176.51 | 2713.72 | 0.02 | 0.02 |\n",
              "| 2 | 350.82 | 3556.20 | 0.02 | 0.03 |\n",
              "| 4 | 686.61 | 3581.65 | 0.02 | 0.05 |\n",
              "| 8 | 1327.60 | 4906.68 | 0.02 | 0.08 |\n",
              "| 16 | 2284.32 | 4964.15 | 0.02 | 0.15 |\n",
              "| 32 | 3307.78 | 6052.02 | 0.02 | 0.24 |\n",
              "| 64 | 3933.59 | 5430.29 | 0.01 | 0.53 |\n",
              "| 128 | 4333.86 | 6620.12 | 0.01 | 0.87 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5539\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 1.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 61.08 | 5805.73 | 0.97 | 0.97 |\n",
              "| 2 | 98.14 | 6489.22 | 0.91 | 1.73 |\n",
              "| 4 | 185.56 | 6924.31 | 0.92 | 3.24 |\n",
              "| 8 | 195.81 | 7054.24 | 0.96 | 6.37 |\n",
              "| 16 | 257.38 | 7292.93 | 0.95 | 12.28 |\n",
              "\n",
              "### FP16 Test Results\n",
              "\n",
              "#### 3070 laptop\n",
              "\n",
              "##### vllm xformer\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 4.00 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 191.58 | 396.31 | 0.12 | 0.12 |\n",
              "| 2 | 188.85 | 1373.13 | 0.07 | 0.07 |\n",
              "| 4 | 567.70 | 2502.72 | 0.07 | 0.08 |\n",
              "| 8 | 1079.86 | 4669.36 | 0.07 | 0.08 |\n",
              "| 16 | 1677.36 | 5520.68 | 0.06 | 0.13 |\n",
              "| 32 | 2931.42 | 8041.26 | 0.13 | 0.18 |\n",
              "| 64 | 4236.04 | 9555.88 | 0.11 | 0.31 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5633\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 2.60 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 103.62 | 23042.82 | 0.25 | 0.25 |\n",
              "| 2 | 50.94 | 23025.27 | 0.28 | 0.49 |\n",
              "| 4 | 194.34 | 23268.16 | 0.32 | 0.97 |\n",
              "| 8 | 369.72 | 23895.54 | 0.32 | 1.88 |\n",
              "| 16 | 563.25 | 24113.27 | 0.34 | 3.74 |\n",
              "\n",
              "##### vllm fa2\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 3.60 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 187.29 | 316.02 | 0.15 | 0.15 |\n",
              "| 2 | 194.00 | 1993.44 | 0.04 | 0.05 |\n",
              "| 4 | 613.77 | 2352.95 | 0.03 | 0.08 |\n",
              "| 8 | 653.82 | 5531.07 | 0.05 | 0.07 |\n",
              "| 16 | 2119.64 | 5437.47 | 0.03 | 0.14 |\n",
              "| 32 | 3341.71 | 8709.83 | 0.09 | 0.17 |\n",
              "| 64 | 4112.55 | 9600.50 | 0.06 | 0.30 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5581\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 2.60 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 129.81 | 23107.58 | 0.24 | 0.24 |\n",
              "| 2 | 133.18 | 23550.33 | 0.28 | 0.48 |\n",
              "| 4 | 124.44 | 24315.44 | 0.28 | 0.93 |\n",
              "| 8 | 375.86 | 23998.11 | 0.30 | 1.87 |\n",
              "| 16 | 434.18 | 24096.61 | 0.33 | 3.74 |\n",
              "\n",
              "##### lmdeploy turbomind\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 2.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 154.77 | 2238.66 | 0.02 | 0.02 |\n",
              "| 2 | 157.46 | 2653.18 | 0.02 | 0.04 |\n",
              "| 4 | 309.74 | 2791.23 | 0.02 | 0.07 |\n",
              "| 8 | 578.12 | 6417.18 | 0.02 | 0.06 |\n",
              "| 16 | 1439.22 | 6484.35 | 0.01 | 0.11 |\n",
              "| 32 | 2814.97 | 9551.40 | 0.02 | 0.15 |\n",
              "| 64 | 4251.23 | 10272.75 | 0.02 | 0.28 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5657\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 3.00 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 123.73 | 19862.84 | 0.29 | 0.29 |\n",
              "| 2 | 107.36 | 21603.39 | 0.29 | 0.53 |\n",
              "| 4 | 134.85 | 22812.97 | 0.31 | 0.99 |\n",
              "| 8 | 335.80 | 23905.97 | 0.30 | 1.89 |\n",
              "| 16 | 288.97 | 24434.42 | 0.34 | 3.68 |\n",
              "\n",
              "#### 2080ti\n",
              "\n",
              "##### vllm xformer\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 1.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 228.98 | 1092.87 | 0.04 | 0.04 |\n",
              "| 2 | 423.88 | 1288.86 | 0.07 | 0.07 |\n",
              "| 4 | 653.95 | 2368.37 | 0.07 | 0.08 |\n",
              "| 8 | 1051.37 | 4368.51 | 0.08 | 0.08 |\n",
              "| 16 | 2127.28 | 6250.58 | 0.07 | 0.12 |\n",
              "| 32 | 3203.85 | 8748.44 | 0.10 | 0.17 |\n",
              "| 64 | 3942.46 | 10506.13 | 0.09 | 0.28 |\n",
              "| 128 | 4603.73 | 18207.47 | 0.09 | 0.32 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5581\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 1.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 27.15 | 29227.07 | 0.19 | 0.19 |\n",
              "| 2 | 171.79 | 31110.00 | 0.23 | 0.36 |\n",
              "| 4 | 340.59 | 34868.82 | 0.24 | 0.65 |\n",
              "| 8 | 213.44 | 36036.22 | 0.25 | 1.24 |\n",
              "| 16 | 377.33 | 36375.68 | 0.32 | 2.48 |\n",
              "\n",
              "##### lmdeploy turbomind\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 1.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 305.94 | 4199.33 | 0.01 | 0.01 |\n",
              "| 2 | 341.57 | 6731.39 | 0.01 | 0.02 |\n",
              "| 4 | 911.32 | 7832.42 | 0.01 | 0.02 |\n",
              "| 8 | 1551.95 | 7321.06 | 0.02 | 0.05 |\n",
              "| 16 | 3052.74 | 5795.36 | 0.01 | 0.13 |\n",
              "| 32 | 4666.90 | 13364.55 | 0.01 | 0.11 |\n",
              "| 64 | 7360.47 | 13220.15 | 0.02 | 0.22 |\n",
              "| 128 | 9013.05 | 14792.20 | 0.02 | 0.39 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5622\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 1.40 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 244.81 | 18598.96 | 0.31 | 0.31 |\n",
              "| 2 | 271.79 | 28980.70 | 0.29 | 0.39 |\n",
              "| 4 | 158.41 | 30582.23 | 0.26 | 0.73 |\n",
              "| 8 | 659.52 | 35179.13 | 0.26 | 1.28 |\n",
              "| 16 | 501.57 | 39903.70 | 0.30 | 2.26 |\n",
              "\n",
              "Updated on 2025-02-08\n",
              "\n",
              "|LLM](/tags/llm/), |VLLM](/tags/vllm/), |XFORMERS](/tags/xformers/), |FLASH\\_ATTN](/tags/flash_attn/)\n",
              "\n",
              "|Back](javascript:void(0);) | |Home](/)\n",
              "\n",
              "|Is Flash Attention 2 a Significant Improvement? Not Necessarily](/posts/flash_attention_2_underwhelming/ \"Is Flash\n",
              "Attention 2 a Significant Improvement? Not Necessarily\")\n",
              "|LLM API Performance Evaluation Tool Guide](/posts/llm_api_performance_evaluation_tool_guide/ \"LLM API Performance \n",
              "Evaluation Tool Guide\")\n",
              "\n",
              "Please enable JavaScript to view the comments powered by |Telegram Comments](https://comments.app/).\n",
              "\n",
              "2024 - 2025 |Loouis](/) | |CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Re-evaluating: The True Power of Flash Attention 2 - 𝓛𝓸𝓸𝓾𝓲𝓼' 𝓑𝓵𝓸𝓰\n",
              "\n",
              "|𝓛𝓸𝓸𝓾𝓲𝓼](/ \"𝓛𝓸𝓸𝓾𝓲𝓼' 𝓑𝓵𝓸𝓰\")\n",
              "\n",
              "|Posts](/posts/) |Tags](/tags/) |Categories](/categories/) \n",
              "\n",
              "|English简体中文](javascript:void(0); \"Select Language\")\n",
              "\n",
              "|𝓛𝓸𝓸𝓾𝓲𝓼](/ \"𝓛𝓸𝓸𝓾𝓲𝓼' 𝓑𝓵𝓸𝓰\")\n",
              "\n",
              "|Cancel](javascript:void(0);)\n",
              "\n",
              "|Posts](/posts/)|Tags](/tags/)|Categories](/categories/)|English简体中文](javascript:void(0); \"Select Language\")\n",
              "\n",
              "Contents\n",
              "--------\n",
              "\n",
              "Re-evaluating: The True Power of Flash Attention 2\n",
              "==================================================\n",
              "\n",
              "|Loouis](/ \"Author\") included in |AI](/categories/ai/)\n",
              "\n",
              "2025-02-08  1404 words \n",
              " 7 minutes\n",
              "\n",
              "!|/images/wallhaven-01887.png](/svg/loading.min.svg \"/images/wallhaven-01887.png\")\n",
              "\n",
              "Contents\n",
              "\n",
              "* |Test Results](#test-results)\n",
              "  + |AWQ Test Results](#awq-test-results)\n",
              "  + |FP16 Test Results](#fp16-test-results)\n",
              "\n",
              "&gt; This article compares the performance differences of vllm when using xformers and flash attention 2 as the \n",
              "backend attention mechanism.\n",
              "\n",
              "Previously, I wrote an |article](https://pikoo.de/posts/flash_attention_2_underwhelming/) comparing the performance\n",
              "differences of vllm when using xformers and flash attention 2 as backend attention mechanisms. However, after a few\n",
              "days of reconsideration, I felt something was off. So, I conducted several more tests and discovered that flash \n",
              "attention 2 does indeed provide a significant boost. Even with a bandwidth difference of over 60%, the acceleration\n",
              "from flash attention 2 can forcefully equalize the generation throughput. This means that the improvement over \n",
              "xformers is at least 40%, and in some scenarios, it can reach 70%. Hence, this article presents several conclusions\n",
              "without extensive analysis:\n",
              "\n",
              "1. For the 30 series, changing the vllm environment variable `VLLM_ATTENTION_BACKEND=XFORMERS` might not actually \n",
              "use xformers, as the speed is consistent with fa2. However, there is a significant improvement compared to the \n",
              "2080ti.\n",
              "2. Using the lmdeploy turbomind inference engine on the 2080ti can achieve the same performance boost as fa2.\n",
              "3. The lmdeploy results on the 2080ti differ from previous tests, showing significant improvements in both awq and \n",
              "fp16.\n",
              "4. However, the 3070 shows different behavior compared to the above, so the 30 series (and presumably the 40 and 50\n",
              "series) should use vllm, as vllm seems to have specific optimizations for the 30 series and later.\n",
              "5. Depending on parameter ratios, fa2 provides an improvement of approximately 40-70% (for fp16 and awq \n",
              "quantization, respectively).\n",
              "\n",
              "Actual hardware parameters and single-request generation throughput extracted from the test results below:\n",
              "\n",
              "|  | tflops | bandwidth | vllm | lmdeploy | vllm | lmdeploy |\n",
              "| --- | --- | --- | --- | --- | --- | --- |\n",
              "| 3070laptop | 29.7 | 319 | 100 | 102 | 191 | 154 |\n",
              "| 2080ti | 52.5 | 526 | 97 | 176 | 228 | 305 |\n",
              "| ratio |  | 1.65 | 0.97 | 1.73 | 1.19 | 1.98 |\n",
              "\n",
              "Test Results\n",
              "------------\n",
              "\n",
              "### AWQ Test Results\n",
              "\n",
              "#### 3070 laptop\n",
              "\n",
              "##### vllm xformer\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 2.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 100.00 | 951.17 | 0.05 | 0.05 |\n",
              "| 2 | 196.12 | 885.67 | 0.10 | 0.10 |\n",
              "| 4 | 378.49 | 1568.36 | 0.10 | 0.12 |\n",
              "| 8 | 714.94 | 2434.35 | 0.13 | 0.15 |\n",
              "| 16 | 1211.09 | 2184.59 | 0.17 | 0.33 |\n",
              "| 32 | 1682.82 | 3098.94 | 0.14 | 0.47 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5638\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 2.20 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 72.75 | 3724.02 | 1.52 | 1.52 |\n",
              "| 2 | 65.69 | 3461.59 | 2.03 | 3.24 |\n",
              "| 4 | 118.01 | 3355.38 | 2.01 | 6.72 |\n",
              "| 8 | 129.15 | 3542.59 | 1.94 | 12.66 |\n",
              "\n",
              "##### vllm fa2\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 2.40 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 101.52 | 1106.82 | 0.04 | 0.04 |\n",
              "| 2 | 199.29 | 1326.93 | 0.07 | 0.07 |\n",
              "| 4 | 397.01 | 2322.60 | 0.08 | 0.08 |\n",
              "| 8 | 744.77 | 2998.94 | 0.04 | 0.12 |\n",
              "| 16 | 1280.79 | 3332.02 | 0.08 | 0.22 |\n",
              "| 32 | 1880.32 | 3579.14 | 0.05 | 0.40 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5519\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 3.20 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 65.94 | 2966.57 | 1.89 | 1.89 |\n",
              "| 2 | 113.30 | 3612.69 | 1.85 | 3.11 |\n",
              "| 4 | 58.97 | 3635.87 | 1.85 | 6.16 |\n",
              "| 8 | 53.07 | 3576.62 | 1.87 | 12.53 |\n",
              "\n",
              "##### lmdeploy turbomind\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 3.20 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 102.33 | 1162.42 | 0.04 | 0.04 |\n",
              "| 2 | 202.70 | 1441.08 | 0.03 | 0.07 |\n",
              "| 4 | 417.91 | 1724.37 | 0.03 | 0.11 |\n",
              "| 8 | 793.40 | 2305.94 | 0.03 | 0.16 |\n",
              "| 16 | 1365.04 | 3255.68 | 0.03 | 0.22 |\n",
              "| 32 | 1939.21 | 3398.90 | 0.03 | 0.43 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5622\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 2.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 26.89 | 4419.59 | 1.29 | 1.29 |\n",
              "| 2 | 129.84 | 4403.95 | 1.29 | 2.52 |\n",
              "| 4 | 145.21 | 4466.76 | 1.30 | 5.02 |\n",
              "| 8 | 172.47 | 4586.10 | 1.33 | 9.78 |\n",
              "\n",
              "#### 2080ti\n",
              "\n",
              "##### vllm xformer\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 1.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 97.29 | 716.20 | 0.06 | 0.06 |\n",
              "| 2 | 189.25 | 787.34 | 0.11 | 0.12 |\n",
              "| 4 | 368.60 | 1630.50 | 0.11 | 0.11 |\n",
              "| 8 | 673.36 | 2310.67 | 0.11 | 0.16 |\n",
              "| 16 | 1132.99 | 2949.96 | 0.12 | 0.25 |\n",
              "| 32 | 1561.65 | 4154.17 | 0.12 | 0.35 |\n",
              "| 64 | 1653.97 | 4995.70 | 0.14 | 0.58 |\n",
              "| 128 | 1795.95 | 5930.27 | 0.13 | 0.97 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5648\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 1.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 64.25 | 6055.06 | 0.93 | 0.93 |\n",
              "| 2 | 49.60 | 6011.82 | 0.99 | 1.84 |\n",
              "| 4 | 187.93 | 6402.48 | 1.48 | 3.52 |\n",
              "| 8 | 159.36 | 6402.51 | 5.11 | 7.04 |\n",
              "| 16 | 178.05 | 6434.66 | 4.73 | 13.92 |\n",
              "\n",
              "##### lmdeploy turbomind\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 1.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 176.51 | 2713.72 | 0.02 | 0.02 |\n",
              "| 2 | 350.82 | 3556.20 | 0.02 | 0.03 |\n",
              "| 4 | 686.61 | 3581.65 | 0.02 | 0.05 |\n",
              "| 8 | 1327.60 | 4906.68 | 0.02 | 0.08 |\n",
              "| 16 | 2284.32 | 4964.15 | 0.02 | 0.15 |\n",
              "| 32 | 3307.78 | 6052.02 | 0.02 | 0.24 |\n",
              "| 64 | 3933.59 | 5430.29 | 0.01 | 0.53 |\n",
              "| 128 | 4333.86 | 6620.12 | 0.01 | 0.87 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5539\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-3B-Instruct-AWQ\n",
              "Latency: 1.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 61.08 | 5805.73 | 0.97 | 0.97 |\n",
              "| 2 | 98.14 | 6489.22 | 0.91 | 1.73 |\n",
              "| 4 | 185.56 | 6924.31 | 0.92 | 3.24 |\n",
              "| 8 | 195.81 | 7054.24 | 0.96 | 6.37 |\n",
              "| 16 | 257.38 | 7292.93 | 0.95 | 12.28 |\n",
              "\n",
              "### FP16 Test Results\n",
              "\n",
              "#### 3070 laptop\n",
              "\n",
              "##### vllm xformer\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 4.00 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 191.58 | 396.31 | 0.12 | 0.12 |\n",
              "| 2 | 188.85 | 1373.13 | 0.07 | 0.07 |\n",
              "| 4 | 567.70 | 2502.72 | 0.07 | 0.08 |\n",
              "| 8 | 1079.86 | 4669.36 | 0.07 | 0.08 |\n",
              "| 16 | 1677.36 | 5520.68 | 0.06 | 0.13 |\n",
              "| 32 | 2931.42 | 8041.26 | 0.13 | 0.18 |\n",
              "| 64 | 4236.04 | 9555.88 | 0.11 | 0.31 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5633\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 2.60 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 103.62 | 23042.82 | 0.25 | 0.25 |\n",
              "| 2 | 50.94 | 23025.27 | 0.28 | 0.49 |\n",
              "| 4 | 194.34 | 23268.16 | 0.32 | 0.97 |\n",
              "| 8 | 369.72 | 23895.54 | 0.32 | 1.88 |\n",
              "| 16 | 563.25 | 24113.27 | 0.34 | 3.74 |\n",
              "\n",
              "##### vllm fa2\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 3.60 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 187.29 | 316.02 | 0.15 | 0.15 |\n",
              "| 2 | 194.00 | 1993.44 | 0.04 | 0.05 |\n",
              "| 4 | 613.77 | 2352.95 | 0.03 | 0.08 |\n",
              "| 8 | 653.82 | 5531.07 | 0.05 | 0.07 |\n",
              "| 16 | 2119.64 | 5437.47 | 0.03 | 0.14 |\n",
              "| 32 | 3341.71 | 8709.83 | 0.09 | 0.17 |\n",
              "| 64 | 4112.55 | 9600.50 | 0.06 | 0.30 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5581\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 2.60 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 129.81 | 23107.58 | 0.24 | 0.24 |\n",
              "| 2 | 133.18 | 23550.33 | 0.28 | 0.48 |\n",
              "| 4 | 124.44 | 24315.44 | 0.28 | 0.93 |\n",
              "| 8 | 375.86 | 23998.11 | 0.30 | 1.87 |\n",
              "| 16 | 434.18 | 24096.61 | 0.33 | 3.74 |\n",
              "\n",
              "##### lmdeploy turbomind\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 2.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 154.77 | 2238.66 | 0.02 | 0.02 |\n",
              "| 2 | 157.46 | 2653.18 | 0.02 | 0.04 |\n",
              "| 4 | 309.74 | 2791.23 | 0.02 | 0.07 |\n",
              "| 8 | 578.12 | 6417.18 | 0.02 | 0.06 |\n",
              "| 16 | 1439.22 | 6484.35 | 0.01 | 0.11 |\n",
              "| 32 | 2814.97 | 9551.40 | 0.02 | 0.15 |\n",
              "| 64 | 4251.23 | 10272.75 | 0.02 | 0.28 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5657\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 3.00 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 123.73 | 19862.84 | 0.29 | 0.29 |\n",
              "| 2 | 107.36 | 21603.39 | 0.29 | 0.53 |\n",
              "| 4 | 134.85 | 22812.97 | 0.31 | 0.99 |\n",
              "| 8 | 335.80 | 23905.97 | 0.30 | 1.89 |\n",
              "| 16 | 288.97 | 24434.42 | 0.34 | 3.68 |\n",
              "\n",
              "#### 2080ti\n",
              "\n",
              "##### vllm xformer\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 1.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 228.98 | 1092.87 | 0.04 | 0.04 |\n",
              "| 2 | 423.88 | 1288.86 | 0.07 | 0.07 |\n",
              "| 4 | 653.95 | 2368.37 | 0.07 | 0.08 |\n",
              "| 8 | 1051.37 | 4368.51 | 0.08 | 0.08 |\n",
              "| 16 | 2127.28 | 6250.58 | 0.07 | 0.12 |\n",
              "| 32 | 3203.85 | 8748.44 | 0.10 | 0.17 |\n",
              "| 64 | 3942.46 | 10506.13 | 0.09 | 0.28 |\n",
              "| 128 | 4603.73 | 18207.47 | 0.09 | 0.32 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5581\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 1.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 27.15 | 29227.07 | 0.19 | 0.19 |\n",
              "| 2 | 171.79 | 31110.00 | 0.23 | 0.36 |\n",
              "| 4 | 340.59 | 34868.82 | 0.24 | 0.65 |\n",
              "| 8 | 213.44 | 36036.22 | 0.25 | 1.24 |\n",
              "| 16 | 377.33 | 36375.68 | 0.32 | 2.48 |\n",
              "\n",
              "##### lmdeploy turbomind\n",
              "\n",
              "```\n",
              "Input Tokens: 45\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 1.80 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 305.94 | 4199.33 | 0.01 | 0.01 |\n",
              "| 2 | 341.57 | 6731.39 | 0.01 | 0.02 |\n",
              "| 4 | 911.32 | 7832.42 | 0.01 | 0.02 |\n",
              "| 8 | 1551.95 | 7321.06 | 0.02 | 0.05 |\n",
              "| 16 | 3052.74 | 5795.36 | 0.01 | 0.13 |\n",
              "| 32 | 4666.90 | 13364.55 | 0.01 | 0.11 |\n",
              "| 64 | 7360.47 | 13220.15 | 0.02 | 0.22 |\n",
              "| 128 | 9013.05 | 14792.20 | 0.02 | 0.39 |\n",
              "\n",
              "```\n",
              "Input Tokens: 5622\n",
              "Output Tokens: 512\n",
              "Test Model: Qwen2.5-0.5B-Instruct\n",
              "Latency: 1.40 ms\n",
              "```\n",
              "\n",
              "| Concurrency | Generation Throughput (tokens/s) | Prompt Throughput (tokens/s) | Min TTFT (s) | Max TTFT (s) |\n",
              "| --- | --- | --- | --- | --- |\n",
              "| 1 | 244.81 | 18598.96 | 0.31 | 0.31 |\n",
              "| 2 | 271.79 | 28980.70 | 0.29 | 0.39 |\n",
              "| 4 | 158.41 | 30582.23 | 0.26 | 0.73 |\n",
              "| 8 | 659.52 | 35179.13 | 0.26 | 1.28 |\n",
              "| 16 | 501.57 | 39903.70 | 0.30 | 2.26 |\n",
              "\n",
              "Updated on 2025-02-08\n",
              "\n",
              "|LLM](/tags/llm/), |VLLM](/tags/vllm/), |XFORMERS](/tags/xformers/), |FLASH\\_ATTN](/tags/flash_attn/)\n",
              "\n",
              "|Back](javascript:void(0);) | |Home](/)\n",
              "\n",
              "|Is Flash Attention 2 a Significant Improvement? Not Necessarily](/posts/flash_attention_2_underwhelming/ \"Is Flash\n",
              "Attention 2 a Significant Improvement? Not Necessarily\")\n",
              "|LLM API Performance Evaluation Tool Guide](/posts/llm_api_performance_evaluation_tool_guide/ \"LLM API Performance \n",
              "Evaluation Tool Guide\")\n",
              "\n",
              "Please enable JavaScript to view the comments powered by |Telegram Comments](https://comments.app/).\n",
              "\n",
              "2024 - 2025 |Loouis](/) | |CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Observations: flash-attention vs xformers - compare differences and reviews? | LibHunt\n",
              "\n",
              "|!|LibHunt \n",
              "logo](https://cdn-b.libhunt.com/assets/logo/logo-square-59c7e305a0cf44062d1ee926560b6384cfb5b175590450cf104dc46b371\n",
              "0ed62.png)\n",
              "LibHunt](/)\n",
              "|!|](https://cdn-b.libhunt.com/tech-icons/file_type_python.svg?v=12.10.0-1)](/l/python)\n",
              "\n",
              "|Topics](/l/python/topics)\n",
              "|Trending](/l/python/trending)\n",
              "|Popularity\n",
              "Index](/index#python)\n",
              "|Add a project](/repo/submit \"Add a project\")\n",
              "|About](/about)\n",
              "\n",
              "1. |Python](/l/python)\n",
              "2. |flash-attention](/r/flash-attention)\n",
              "3. |xformers](/r/xformers)\n",
              "\n",
              "flash-attention\n",
              "VS\n",
              "xformers\n",
              "===========================\n",
              "\n",
              "Compare flash-attention vs xformers and see what are their differences.\n",
              "-----------------------------------------------------------------------\n",
              "\n",
              "|!|Dao-AILab logo](https://avatars.githubusercontent.com/u/139507659?v=4&s=160)](/r/flash-attention)\n",
              "\n",
              "|flash-attention](/r/flash-attention)\n",
              "-------------------------------------\n",
              "\n",
              "Fast and memory-efficient exact attention (by Dao-AILab)\n",
              "\n",
              "|Suggest topics](/repo/3598490/edit)\n",
              "\n",
              "|Source Code](https://github.com/Dao-AILab/flash-attention)\n",
              "\n",
              "|Suggest alternative](/repo/3598490/suggest_alternative)\n",
              "\n",
              "|Edit details](/repo/3598490/edit)\n",
              "\n",
              "|!|facebookresearch logo](https://avatars.githubusercontent.com/u/16943930?v=4&s=160)](/r/xformers)\n",
              "\n",
              "|xformers](/r/xformers)\n",
              "-----------------------\n",
              "\n",
              "Hackable and optimized Transformers building blocks, supporting a composable construction. (by facebookresearch)\n",
              "\n",
              "|Suggest topics](/repo/3639369/edit)\n",
              "\n",
              "|Source Code](https://github.com/facebookresearch/xformers)\n",
              "\n",
              "|facebookresearch.github.io](https://facebookresearch.github.io/xformers/)\n",
              "\n",
              "|Suggest alternative](/repo/3639369/suggest_alternative)\n",
              "\n",
              "|Edit details](/repo/3639369/edit)\n",
              "\n",
              "!|](https://cdn-b.libhunt.com/images/promo-campaign-images/000/000/021/main.png?1663900378 \"InfluxDB Logo\")\n",
              "\n",
              "**InfluxDB – Built for High-Performance Time Series Workloads**\n",
              "\n",
              "InfluxDB 3 OSS is now GA. Transform, enrich, and act on time series data directly in the database. Automate \n",
              "critical tasks and eliminate the need to move data externally. Download now.\n",
              "\n",
              "www.influxdata.com\n",
              "\n",
              "featured\n",
              "\n",
              "!|](https://cdn-b.libhunt.com/images/promo-campaign-images/000/000/048/main.png?1753729821 \"Sevalla Logo\")\n",
              "\n",
              "**Sevalla - Deploy and host your apps and databases, now with $50 credit!**\n",
              "\n",
              "Sevalla is the PaaS you have been looking for! Advanced deployment pipelines, usage-based pricing, preview apps, \n",
              "templates, human support by developers, and much more!\n",
              "\n",
              "sevalla.com\n",
              "\n",
              "featured\n",
              "\n",
              "| |flash-attention](/r/flash-attention) |  | |xformers](/r/xformers) |\n",
              "| --- | --- | --- |\n",
              "|  | Project |  |\n",
              "| 29 | Mentions | 48 |\n",
              "| 19,335 | Stars | 9,912 |\n",
              "| 2.9% | Growth | 1.3% |\n",
              "| 9.6 | Activity | 9.2 |\n",
              "| 4 days ago | Latest Commit | 15 days ago |\n",
              "| Python | Language | Python |\n",
              "| BSD 3-clause \"New\" or \"Revised\" License | License | GNU General Public License v3.0 or later |\n",
              "\n",
              "The number of **mentions** indicates the total number of mentions that we've tracked plus the number of user \n",
              "suggested alternatives.\n",
              "  \n",
              "**Stars** - the number of stars that a project has on GitHub.\n",
              "**Growth** - month over month growth in stars.\n",
              "  \n",
              "**Activity** is a relative number indicating how actively a project is being developed.\n",
              "Recent commits have higher weight than older ones.\n",
              "  \n",
              "For example, an activity of **9.0** indicates that a project is amongst the top 10%\n",
              "of the most actively developed projects that we are tracking.\n",
              "\n",
              "flash-attention\n",
              "---------------\n",
              "\n",
              "Posts with mentions or **reviews of flash-attention**.\n",
              "We have used some of these posts to build our list of alternatives\n",
              "and similar projects. The last one was on 2025-09-05.\n",
              "\n",
              "* |Why ML Needs a New Programming Language](/posts/1446995-why-ml-needs-a-new-programming-language)\n",
              "\n",
              "  12 projects\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=45137373)\n",
              "  |\n",
              "  5 Sep 2025\n",
              "\n",
              "  > IIUC, triton uses Python syntax, but it has a separate compiler (which is kinda what Mojo is doing, except \n",
              "Mojo's syntax is a superset of Python's, instead of a subset, like Triton). I think it's fair to describe it as a \n",
              "different language. Triton describes itself as \"the Triton language and compiler\" (as opposed to, I dunno, \"Write \n",
              "GPU kernels in Python\").\n",
              "  >\n",
              "  > Also, flash attention is at v3-beta right now? |0] And it requires one of CUDA/Triton/ROCm?\n",
              "  >\n",
              "  > |0] https://github.com/Dao-AILab/flash-attention\n",
              "  >\n",
              "  > But maybe I'm out of the loop? Where do you see that flash attention 4 is written in Python?\n",
              "* |Uv format: Code Formatting Comes to uv \n",
              "(experimentally)](/posts/1444715-uv-format-code-formatting-comes-to-uv-experimentally)\n",
              "\n",
              "  4 projects\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=44977645)\n",
              "  |\n",
              "  21 Aug 2025\n",
              "\n",
              "  > 3) Call into the kernels with pure Python, without going through a C++ extension.\n",
              "  >\n",
              "  > I do this for the CUDA kernels I maintain and it works great.\n",
              "  >\n",
              "  > Flash attention currently publishes 48 (!) different packages|1], for different combinations of pytorch and C++\n",
              "ABI. With this approach it would have to only publish *one*, and it would work for every combination of Python and \n",
              "pytorch.\n",
              "  >\n",
              "  > |1] - https://github.com/Dao-AILab/flash-attention/releases/tag/v2...\n",
              "* |FlashAttention-3: Fast and Accurate Attention with Asynchrony and \n",
              "Low-Precision](/posts/1380403-flashattention-3-fast-and-accurate-attention-with-asynchrony-and-low-precision)\n",
              "\n",
              "  5 projects\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=40938577)\n",
              "  |\n",
              "  11 Jul 2024\n",
              "\n",
              "  > 1) Pretty much, it's mathematically equivalent. The only software issues are things like managing dependency \n",
              "versions and data formats in-memory, but Flash Attention 2 is already built into HuggingFace and other popular \n",
              "libraries. Flash Attention 3 probably will be soon, although it requires an H100 GPU to run\n",
              "  >\n",
              "  > 2) Flash Attention 2 added support for GQA in past version updates:\n",
              "  >\n",
              "  > https://github.com/Dao-AILab/flash-attention\n",
              "  >\n",
              "  > 3) They're comparing this implementation of Flash Attention (which is written in raw CUDA C++) to the Triton \n",
              "implementation of a similar algorithm (which is written in Triton): \n",
              "https://triton-lang.org/main/getting-started/tutorials/06-fu...\n",
              "* |How the Transformer Architecture Was Likely Discovered: A Step-by-Step \n",
              "Guide](/posts/1366977-how-the-transformer-architecture-was-likely-discovered-a-step-by-step-guide)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |dev.to](https://dev.to/freiberg-roman/how-the-transformer-architecture-was-likely-discovered-a-step-by-step-guid\n",
              "e-36hd)\n",
              "  |\n",
              "  8 Apr 2024\n",
              "\n",
              "  > If you're looking for an implementation, I highly recommend checking out fast attention \n",
              "|https://github.com/Dao-AILab/flash-attention]. It's my go-to, and far better than anything we could whip up here \n",
              "using just PyTorch or TensorFlow.\n",
              "* |Interactive Coloring with ControlNet](/posts/1360738-interactive-coloring-with-controlnet)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=39413652)\n",
              "  |\n",
              "  17 Feb 2024\n",
              "\n",
              "  > \\* Even if I bought a 3090, I would have to get a computer to go with it, along with a PSU and some cooling. \n",
              "Don't know where to start with that.\n",
              "  >\n",
              "  > |1] https://github.com/Dao-AILab/flash-attention/issues/190\n",
              "* |Coding Self-Attention, Multi-Head Attention, Cross-Attention, \n",
              "Causal-Attention](/posts/1356458-coding-self-attention-multi-head-attention-cross-attention-causal-attention)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=38990709)\n",
              "  |\n",
              "  14 Jan 2024\n",
              "\n",
              "  > highly recommend using Tri's implementation https://github.com/Dao-AILab/flash-attention rotary should be built\n",
              "in, and some group overseas even contributed alibi\n",
              "* |PSA: new ExLlamaV2 quant method makes 70Bs perform much better at low bpw \n",
              "quants](/posts/1350493-psa-new-exllamav2-quant-method-makes-70bs-perform-much-better-at-low-bpw-quants)\n",
              "\n",
              "  2 projects\n",
              "  |\n",
              "  |/r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/18eyf39/psa_new_exllamav2_quant_method_makes_70bs_pe\n",
              "rform/)\n",
              "  |\n",
              "  10 Dec 2023\n",
              "\n",
              "  > Doesn't seem so\n",
              "  > https://github.com/Dao-AILab/flash-attention/issues/542\n",
              "  > No updates for a while.\n",
              "* |VLLM: 24x faster LLM serving than HuggingFace \n",
              "Transformers](/posts/1280695-vllm-24x-faster-llm-serving-than-huggingface-transformers)\n",
              "\n",
              "  3 projects\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=36409082)\n",
              "  |\n",
              "  20 Jun 2023\n",
              "\n",
              "  > I wonder how this compares to Flash Attention (https://github.com/HazyResearch/flash-attention), which is the \n",
              "other \"memory aware\" Attention project I'm aware of.\n",
              "  >\n",
              "  > I guess Flash Attention is more about utilizing memory GPU SRam correctly, where this is more about using the \n",
              "OS/CPU memory better?\n",
              "* |Hacking Around ChatGPT’s Character Limits with the Code \n",
              "Interpreter](/posts/1249820-hacking-around-chatgpt-s-character-limits-with-the-code-interpreter)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=36098511)\n",
              "  |\n",
              "  27 May 2023\n",
              "\n",
              "  > https://github.com/HazyResearch/flash-attention\n",
              "* |Flash Attention on Consumer](/posts/1224566-flash-attention-on-consumer)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |/r/LocalLLM](https://www.reddit.com/r/LocalLLM/comments/13eb44f/flash_attention_on_consumer/)\n",
              "  |\n",
              "  10 May 2023\n",
              "\n",
              "xformers\n",
              "--------\n",
              "\n",
              "Posts with mentions or **reviews of xformers**.\n",
              "We have used some of these posts to build our list of alternatives\n",
              "and similar projects. The last one was on 2024-11-13.\n",
              "\n",
              "* |Practical Experience: Integrating Over 50 Neural Networks Into One Open-Source \n",
              "Project](/posts/1399810-practical-experience-integrating-over-50-neural-networks-into-one-open-source-project)\n",
              "\n",
              "  3 projects\n",
              "  |\n",
              "  |dev.to](https://dev.to/radchenko/practical-experience-integrating-over-50-neural-networks-into-one-open-source-p\n",
              "roject-1jn5)\n",
              "  |\n",
              "  13 Nov 2024\n",
              "\n",
              "  > Check xformers Compatibility\n",
              "  > Visit the xformers GitHub repo to ensure compatibility with your torch and CUDA versions. Support for older \n",
              "versions can be dropped, so staying updated is vital, especially if you're running CUDA 11.8 and want to leverage \n",
              "xformers for limited VRAM.\n",
              "* |An Interview with AMD CEO Lisa Su About Solving Hard \n",
              "Problems](/posts/1376884-an-interview-with-amd-ceo-lisa-su-about-solving-hard-problems)\n",
              "\n",
              "  8 projects\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=40697341)\n",
              "  |\n",
              "  17 Jun 2024\n",
              "* |Animediff error](/posts/1334054-animediff-error)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |/r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/17jpsza/animediff_error/)\n",
              "  |\n",
              "  31 Oct 2023\n",
              "* |Colab | Errors when installing x-formers](/posts/1330083-colab-errors-when-installing-x-formers)\n",
              "\n",
              "  2 projects\n",
              "  |\n",
              "  |/r/comfyui](https://www.reddit.com/r/comfyui/comments/177jmtq/colab_errors_when_installing_xformers/)\n",
              "  |\n",
              "  15 Oct 2023\n",
              "\n",
              "  > ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This\n",
              "behaviour is the source of the following dependency conflicts.\n",
              "  > fastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.1.0+cu118 which is incompatible.\n",
              "  > torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.1.0+cu118 which is incompatible.\n",
              "  > torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.1.0+cu118 which is incompatible.\n",
              "  > torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.1.0+cu118 which is incompatible.\n",
              "  > torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.1.0+cu118 which is incompatible.\n",
              "  > WARNING|XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
              "  > PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.1.0+cu118)\n",
              "  > Python 3.10.13 (you have 3.10.12)\n",
              "  > Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
              "  > Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
              "  > Set XFORMERS\\_MORE\\_DETAILS=1 for more details\n",
              "  > xformers version: 0.0.22.post3\n",
              "* |FlashAttention-2, 2x faster than FlashAttention](/posts/1308087-flashattention-2-2x-faster-than-flashattention)\n",
              "\n",
              "  3 projects\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=36761988)\n",
              "  |\n",
              "  17 Jul 2023\n",
              "\n",
              "  > This enables V1. V2 is still yet to be integrated into xformers. The team replied saying it should happen this \n",
              "week.\n",
              "  >\n",
              "  > See the relevant Github issue here: https://github.com/facebookresearch/xformers/issues/795\n",
              "* |Xformers issue](/posts/1306864-xformers-issue)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |/r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/14yef7y/xformers_issue/)\n",
              "  |\n",
              "  13 Jul 2023\n",
              "\n",
              "  > My Xformers doesnt work, any help see code. info ( Exception training model: 'Refer to \n",
              "https://github.com/facebookresearch/xformers for more information on how to install xformers'. ) or\n",
              "* |Having xformer troubles](/posts/1299537-having-xformer-troubles)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |/r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/14stuvj/having_xformer_troubles/)\n",
              "  |\n",
              "  6 Jul 2023\n",
              "\n",
              "  > ModuleNotFoundError: Refer to https://github.com/facebookresearch/xformers for more\n",
              "* |Question: these 4 crappy picture have been generated with the same seed and settings. Why they keep coming \n",
              "mildly \n",
              "different?](/posts/1264111-question-these-4-crappy-picture-have-been-generated-with-the-same-seed-and-settings-why-\n",
              "they-keep-coming-mildly-different)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |/r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/142thnq/question_these_4_crappy_picture_ha\n",
              "ve_been/)\n",
              "  |\n",
              "  6 Jun 2023\n",
              "\n",
              "  > Xformers is a module that that can be used with Stable Diffusion. It decreases the memory required to generate \n",
              "an image as well as speeding things up. It works very well but there are two problems with Xformers:\n",
              "* |Stuck trying to update xformers](/posts/1231891-stuck-trying-to-update-xformers)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |/r/SDtechsupport](https://www.reddit.com/r/SDtechsupport/comments/13ixgky/stuck_trying_to_update_xformers/)\n",
              "  |\n",
              "  15 May 2023\n",
              "\n",
              "  > WARNING|XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
              "  > PyTorch 1.13.1+cu117 with CUDA 1107 (you have 2.0.1+cu118)\n",
              "  > Python 3.10.9 (you have 3.10.7)\n",
              "  > Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
              "  > Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
              "  > Set XFORMERS\\_MORE\\_DETAILS=1 for more details\n",
              "  > =================================================================================\n",
              "  > You are running xformers 0.0.16rc425.\n",
              "  > The program is tested to work with xformers 0.0.17.\n",
              "  > To reinstall the desired version, run with commandline flag --reinstall-xformers.\n",
              "  > Use --skip-version-check commandline argument to disable this check.\n",
              "  > =================================================================================\n",
              "* |Question about updating Xformers for A1111](/posts/1207625-question-about-updating-xformers-for-a1111)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |/r/SDtechsupport](https://www.reddit.com/r/SDtechsupport/comments/133a93u/question_about_updating_xformers_for_a\n",
              "1111/)\n",
              "  |\n",
              "  29 Apr 2023\n",
              "\n",
              "  > # Your version of xformers is 0.0.16rc425.\n",
              "  > # xformers >= 0.0.17.dev is required to be available on the Dreambooth tab.\n",
              "  > # Torch 1 wheels of xformers >= 0.0.17.dev are no longer available on PyPI,\n",
              "  > # but you can manually download them by going to:\n",
              "  > https://github.com/facebookresearch/xformers/actions\n",
              "  > # Click on the most recent action tagged with a release (middle column).\n",
              "  > # Select a download based on your environment.\n",
              "  > # Unzip your download\n",
              "  > # Activate your venv and install the wheel: (from A1111 project root)\n",
              "  > cd venv/Scripts\n",
              "  > activate\n",
              "  > pip install {REPLACE WITH PATH TO YOUR UNZIPPED .whl file}\n",
              "  > # Then restart your project.\n",
              "\n",
              "What are some alternatives?\n",
              "---------------------------\n",
              "\n",
              "When comparing flash-attention and xformers you can also consider the following projects:\n",
              "\n",
              "**TensorRT**\n",
              "- NVIDIA® TensorRT™ is an SDK for high-performance deep learning inference on NVIDIA GPUs. This repository contains\n",
              "the open source components of TensorRT.\n",
              "\n",
              "**Dreambooth-Stable-Diffusion**\n",
              "- Implementation of Dreambooth (https://arxiv.org/abs/2208.12242) with Stable Diffusion\n",
              "\n",
              "**RWKV-LM**\n",
              "- RWKV (pronounced RwaKuv) is an RNN with great LLM performance, which can also be directly trained like a GPT \n",
              "transformer (parallelizable). We are at RWKV-7 \"Goose\". So it's combining the best of RNN and transformer - great \n",
              "performance, linear time, constant space (no kv-cache), fast training, infinite ctx\\_len, and free sentence \n",
              "embedding.\n",
              "\n",
              "**SHARK-Studio**\n",
              "- SHARK Studio -- Web UI for SHARK+IREE High Performance Machine Learning Distribution\n",
              "\n",
              "**DeepSpeed**\n",
              "- DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, \n",
              "and effective.\n",
              "\n",
              "**diffusers**\n",
              "- 🤗 Diffusers: State-of-the-art diffusion models for image and audio generation in PyTorch\n",
              "\n",
              "|flash-attention vs TensorRT](/compare-flash-attention-vs-TensorRT)\n",
              "|xformers vs Dreambooth-Stable-Diffusion](/compare-xformers-vs-Dreambooth-Stable-Diffusion)\n",
              "|flash-attention vs RWKV-LM](/compare-flash-attention-vs-RWKV-LM)\n",
              "|xformers vs SHARK-Studio](/compare-xformers-vs-SHARK-Studio)\n",
              "|flash-attention vs DeepSpeed](/compare-flash-attention-vs-DeepSpeed)\n",
              "|xformers vs diffusers](/compare-xformers-vs-ShivamShrirao--diffusers)\n",
              "\n",
              "!|](https://cdn-b.libhunt.com/images/promo-campaign-images/000/000/021/main.png?1663900378 \"InfluxDB Logo\")\n",
              "\n",
              "**InfluxDB – Built for High-Performance Time Series Workloads**\n",
              "\n",
              "InfluxDB 3 OSS is now GA. Transform, enrich, and act on time series data directly in the database. Automate \n",
              "critical tasks and eliminate the need to move data externally. Download now.\n",
              "\n",
              "www.influxdata.com\n",
              "\n",
              "featured\n",
              "\n",
              "!|](https://cdn-b.libhunt.com/images/promo-campaign-images/000/000/048/main.png?1753729821 \"Sevalla Logo\")\n",
              "\n",
              "**Sevalla - Deploy and host your apps and databases, now with $50 credit!**\n",
              "\n",
              "Sevalla is the PaaS you have been looking for! Advanced deployment pipelines, usage-based pricing, preview apps, \n",
              "templates, human support by developers, and much more!\n",
              "\n",
              "sevalla.com\n",
              "\n",
              "featured\n",
              "\n",
              "Do not miss the trending **Python projects** with our **weekly report!**\n",
              "\n",
              "### Did you know that **Python** is |the 2nd most popular programming language](/index#python) based on number of \n",
              "references?\n",
              "\n",
              "About\n",
              "\n",
              "LibHunt tracks mentions of software libraries on relevant social networks.\n",
              "Based on that data, you can find the most popular open-source packages,\n",
              "as well as similar and alternative projects.\n",
              "This is our **Awesome Python** list.\n",
              "\n",
              "Sponsored by\n",
              "\n",
              "|!|Sevalla logo](https://cdn-b.libhunt.com/images/promo-campaign-icons/000/000/048/icon.png?1753729822)\n",
              "Sevalla](https://sevalla.com/?utm_source=libhunt&utm_medium=Referral&utm_campaign=website)|!|InfluxDB \n",
              "logo](https://cdn-b.libhunt.com/images/promo-campaign-icons/000/000/021/icon.png?1678174694)\n",
              "InfluxDB](https://www.influxdata.com/products/influxdb3/?utm_source=vendor&utm_medium=referral&utm_campaign=2025_sp\n",
              "nsr-web_website-content_libhunt&utm_content=web)|!|SaaSHub \n",
              "logo](https://cdn-b.libhunt.com/assets/partners/saashub-icon-195c856d3186551a610929bfac8bc312607331a5410cc6fb196c91\n",
              "acfb13f42a.png)\n",
              "SaaSHub](https://www.saashub.com)\n",
              "\n",
              "|Become a sponsor](/advertise)\n",
              "\n",
              "Sitemap\n",
              "\n",
              "* |About](/about)\n",
              "* |Popularity Index](/index)\n",
              "* |Site Generators](/ssg)\n",
              "* |CSS Frameworks](/css)\n",
              "* |JS Frameworks](/jsf)\n",
              "* |CMS Platforms](/cms)\n",
              "* |Databases](/dbs)\n",
              "* |/DEVs](/devs)\n",
              "* |Python Devs](/l/python/devs)\n",
              "* |RSS](https://www.libhunt.com/l/python/feed)\n",
              "\n",
              "Tools\n",
              "\n",
              "* |Add a project](/repo/submit)\n",
              "* |Find alternatives](/site/find_alternatives)\n",
              "\n",
              "Contacts\n",
              "\n",
              "* |Feedback?](/feedback)\n",
              "* |Email](/about#contacts)\n",
              "* |!|LibHunt \n",
              "badge](https://cdn-b.saashub.com/img/badges/featured-color.png?v=1)](https://www.saashub.com/libhunt?utm_source=bad\n",
              "ge&badge_variant=color&badge_kind=featured)\n",
              "\n",
              "LibHunt\n",
              "|\n",
              "|Privacy Policy](/privacy)\n",
              "|\n",
              "|Terms](/terms)\n",
              "|\n",
              "CC BY-SA 4.0\n",
              "\n",
              "We recommend\n",
              "|Spin The Wheel Of Names](https://spinthewheelofnames.com)\n",
              "for a cryptographically secure random name picker.\n",
              "\n",
              "Loading...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: flash-attention vs xformers - compare differences and reviews? | LibHunt\n",
              "\n",
              "|!|LibHunt \n",
              "logo](https://cdn-b.libhunt.com/assets/logo/logo-square-59c7e305a0cf44062d1ee926560b6384cfb5b175590450cf104dc46b371\n",
              "0ed62.png)\n",
              "LibHunt](/)\n",
              "|!|](https://cdn-b.libhunt.com/tech-icons/file_type_python.svg?v=12.10.0-1)](/l/python)\n",
              "\n",
              "|Topics](/l/python/topics)\n",
              "|Trending](/l/python/trending)\n",
              "|Popularity\n",
              "Index](/index#python)\n",
              "|Add a project](/repo/submit \"Add a project\")\n",
              "|About](/about)\n",
              "\n",
              "1. |Python](/l/python)\n",
              "2. |flash-attention](/r/flash-attention)\n",
              "3. |xformers](/r/xformers)\n",
              "\n",
              "flash-attention\n",
              "VS\n",
              "xformers\n",
              "===========================\n",
              "\n",
              "Compare flash-attention vs xformers and see what are their differences.\n",
              "-----------------------------------------------------------------------\n",
              "\n",
              "|!|Dao-AILab logo](https://avatars.githubusercontent.com/u/139507659?v=4&amp;s=160)](/r/flash-attention)\n",
              "\n",
              "|flash-attention](/r/flash-attention)\n",
              "-------------------------------------\n",
              "\n",
              "Fast and memory-efficient exact attention (by Dao-AILab)\n",
              "\n",
              "|Suggest topics](/repo/3598490/edit)\n",
              "\n",
              "|Source Code](https://github.com/Dao-AILab/flash-attention)\n",
              "\n",
              "|Suggest alternative](/repo/3598490/suggest_alternative)\n",
              "\n",
              "|Edit details](/repo/3598490/edit)\n",
              "\n",
              "|!|facebookresearch logo](https://avatars.githubusercontent.com/u/16943930?v=4&amp;s=160)](/r/xformers)\n",
              "\n",
              "|xformers](/r/xformers)\n",
              "-----------------------\n",
              "\n",
              "Hackable and optimized Transformers building blocks, supporting a composable construction. (by facebookresearch)\n",
              "\n",
              "|Suggest topics](/repo/3639369/edit)\n",
              "\n",
              "|Source Code](https://github.com/facebookresearch/xformers)\n",
              "\n",
              "|facebookresearch.github.io](https://facebookresearch.github.io/xformers/)\n",
              "\n",
              "|Suggest alternative](/repo/3639369/suggest_alternative)\n",
              "\n",
              "|Edit details](/repo/3639369/edit)\n",
              "\n",
              "!|](https://cdn-b.libhunt.com/images/promo-campaign-images/000/000/021/main.png?1663900378 \"InfluxDB Logo\")\n",
              "\n",
              "**InfluxDB – Built for High-Performance Time Series Workloads**\n",
              "\n",
              "InfluxDB 3 OSS is now GA. Transform, enrich, and act on time series data directly in the database. Automate \n",
              "critical tasks and eliminate the need to move data externally. Download now.\n",
              "\n",
              "www.influxdata.com\n",
              "\n",
              "featured\n",
              "\n",
              "!|](https://cdn-b.libhunt.com/images/promo-campaign-images/000/000/048/main.png?1753729821 \"Sevalla Logo\")\n",
              "\n",
              "**Sevalla - Deploy and host your apps and databases, now with $50 credit!**\n",
              "\n",
              "Sevalla is the PaaS you have been looking for! Advanced deployment pipelines, usage-based pricing, preview apps, \n",
              "templates, human support by developers, and much more!\n",
              "\n",
              "sevalla.com\n",
              "\n",
              "featured\n",
              "\n",
              "| |flash-attention](/r/flash-attention) |  | |xformers](/r/xformers) |\n",
              "| --- | --- | --- |\n",
              "|  | Project |  |\n",
              "| 29 | Mentions | 48 |\n",
              "| 19,335 | Stars | 9,912 |\n",
              "| 2.9% | Growth | 1.3% |\n",
              "| 9.6 | Activity | 9.2 |\n",
              "| 4 days ago | Latest Commit | 15 days ago |\n",
              "| Python | Language | Python |\n",
              "| BSD 3-clause \"New\" or \"Revised\" License | License | GNU General Public License v3.0 or later |\n",
              "\n",
              "The number of **mentions** indicates the total number of mentions that we've tracked plus the number of user \n",
              "suggested alternatives.\n",
              "  \n",
              "**Stars** - the number of stars that a project has on GitHub.\n",
              "**Growth** - month over month growth in stars.\n",
              "  \n",
              "**Activity** is a relative number indicating how actively a project is being developed.\n",
              "Recent commits have higher weight than older ones.\n",
              "  \n",
              "For example, an activity of **9.0** indicates that a project is amongst the top 10%\n",
              "of the most actively developed projects that we are tracking.\n",
              "\n",
              "flash-attention\n",
              "---------------\n",
              "\n",
              "Posts with mentions or **reviews of flash-attention**.\n",
              "We have used some of these posts to build our list of alternatives\n",
              "and similar projects. The last one was on 2025-09-05.\n",
              "\n",
              "* |Why ML Needs a New Programming Language](/posts/1446995-why-ml-needs-a-new-programming-language)\n",
              "\n",
              "  12 projects\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=45137373)\n",
              "  |\n",
              "  5 Sep 2025\n",
              "\n",
              "  &gt; IIUC, triton uses Python syntax, but it has a separate compiler (which is kinda what Mojo is doing, except \n",
              "Mojo's syntax is a superset of Python's, instead of a subset, like Triton). I think it's fair to describe it as a \n",
              "different language. Triton describes itself as \"the Triton language and compiler\" (as opposed to, I dunno, \"Write \n",
              "GPU kernels in Python\").\n",
              "  &gt;\n",
              "  &gt; Also, flash attention is at v3-beta right now? |0] And it requires one of CUDA/Triton/ROCm?\n",
              "  &gt;\n",
              "  &gt; |0] https://github.com/Dao-AILab/flash-attention\n",
              "  &gt;\n",
              "  &gt; But maybe I'm out of the loop? Where do you see that flash attention 4 is written in Python?\n",
              "* |Uv format: Code Formatting Comes to uv \n",
              "(experimentally)](/posts/1444715-uv-format-code-formatting-comes-to-uv-experimentally)\n",
              "\n",
              "  4 projects\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=44977645)\n",
              "  |\n",
              "  21 Aug 2025\n",
              "\n",
              "  &gt; 3) Call into the kernels with pure Python, without going through a C++ extension.\n",
              "  &gt;\n",
              "  &gt; I do this for the CUDA kernels I maintain and it works great.\n",
              "  &gt;\n",
              "  &gt; Flash attention currently publishes 48 (!) different packages|1], for different combinations of pytorch and C++\n",
              "ABI. With this approach it would have to only publish *one*, and it would work for every combination of Python and \n",
              "pytorch.\n",
              "  &gt;\n",
              "  &gt; |1] - https://github.com/Dao-AILab/flash-attention/releases/tag/v2...\n",
              "* |FlashAttention-3: Fast and Accurate Attention with Asynchrony and \n",
              "Low-Precision](/posts/1380403-flashattention-3-fast-and-accurate-attention-with-asynchrony-and-low-precision)\n",
              "\n",
              "  5 projects\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=40938577)\n",
              "  |\n",
              "  11 Jul 2024\n",
              "\n",
              "  &gt; 1) Pretty much, it's mathematically equivalent. The only software issues are things like managing dependency \n",
              "versions and data formats in-memory, but Flash Attention 2 is already built into HuggingFace and other popular \n",
              "libraries. Flash Attention 3 probably will be soon, although it requires an H100 GPU to run\n",
              "  &gt;\n",
              "  &gt; 2) Flash Attention 2 added support for GQA in past version updates:\n",
              "  &gt;\n",
              "  &gt; https://github.com/Dao-AILab/flash-attention\n",
              "  &gt;\n",
              "  &gt; 3) They're comparing this implementation of Flash Attention (which is written in raw CUDA C++) to the Triton \n",
              "implementation of a similar algorithm (which is written in Triton): \n",
              "https://triton-lang.org/main/getting-started/tutorials/06-fu...\n",
              "* |How the Transformer Architecture Was Likely Discovered: A Step-by-Step \n",
              "Guide](/posts/1366977-how-the-transformer-architecture-was-likely-discovered-a-step-by-step-guide)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |dev.to](https://dev.to/freiberg-roman/how-the-transformer-architecture-was-likely-discovered-a-step-by-step-guid\n",
              "e-36hd)\n",
              "  |\n",
              "  8 Apr 2024\n",
              "\n",
              "  &gt; If you're looking for an implementation, I highly recommend checking out fast attention \n",
              "|https://github.com/Dao-AILab/flash-attention]. It's my go-to, and far better than anything we could whip up here \n",
              "using just PyTorch or TensorFlow.\n",
              "* |Interactive Coloring with ControlNet](/posts/1360738-interactive-coloring-with-controlnet)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=39413652)\n",
              "  |\n",
              "  17 Feb 2024\n",
              "\n",
              "  &gt; \\* Even if I bought a 3090, I would have to get a computer to go with it, along with a PSU and some cooling. \n",
              "Don't know where to start with that.\n",
              "  &gt;\n",
              "  &gt; |1] https://github.com/Dao-AILab/flash-attention/issues/190\n",
              "* |Coding Self-Attention, Multi-Head Attention, Cross-Attention, \n",
              "Causal-Attention](/posts/1356458-coding-self-attention-multi-head-attention-cross-attention-causal-attention)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=38990709)\n",
              "  |\n",
              "  14 Jan 2024\n",
              "\n",
              "  &gt; highly recommend using Tri's implementation https://github.com/Dao-AILab/flash-attention rotary should be built\n",
              "in, and some group overseas even contributed alibi\n",
              "* |PSA: new ExLlamaV2 quant method makes 70Bs perform much better at low bpw \n",
              "quants](/posts/1350493-psa-new-exllamav2-quant-method-makes-70bs-perform-much-better-at-low-bpw-quants)\n",
              "\n",
              "  2 projects\n",
              "  |\n",
              "  |/r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/18eyf39/psa_new_exllamav2_quant_method_makes_70bs_pe\n",
              "rform/)\n",
              "  |\n",
              "  10 Dec 2023\n",
              "\n",
              "  &gt; Doesn't seem so\n",
              "  &gt; https://github.com/Dao-AILab/flash-attention/issues/542\n",
              "  &gt; No updates for a while.\n",
              "* |VLLM: 24x faster LLM serving than HuggingFace \n",
              "Transformers](/posts/1280695-vllm-24x-faster-llm-serving-than-huggingface-transformers)\n",
              "\n",
              "  3 projects\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=36409082)\n",
              "  |\n",
              "  20 Jun 2023\n",
              "\n",
              "  &gt; I wonder how this compares to Flash Attention (https://github.com/HazyResearch/flash-attention), which is the \n",
              "other \"memory aware\" Attention project I'm aware of.\n",
              "  &gt;\n",
              "  &gt; I guess Flash Attention is more about utilizing memory GPU SRam correctly, where this is more about using the \n",
              "OS/CPU memory better?\n",
              "* |Hacking Around ChatGPT’s Character Limits with the Code \n",
              "Interpreter](/posts/1249820-hacking-around-chatgpt-s-character-limits-with-the-code-interpreter)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=36098511)\n",
              "  |\n",
              "  27 May 2023\n",
              "\n",
              "  &gt; https://github.com/HazyResearch/flash-attention\n",
              "* |Flash Attention on Consumer](/posts/1224566-flash-attention-on-consumer)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |/r/LocalLLM](https://www.reddit.com/r/LocalLLM/comments/13eb44f/flash_attention_on_consumer/)\n",
              "  |\n",
              "  10 May 2023\n",
              "\n",
              "xformers\n",
              "--------\n",
              "\n",
              "Posts with mentions or **reviews of xformers**.\n",
              "We have used some of these posts to build our list of alternatives\n",
              "and similar projects. The last one was on 2024-11-13.\n",
              "\n",
              "* |Practical Experience: Integrating Over 50 Neural Networks Into One Open-Source \n",
              "Project](/posts/1399810-practical-experience-integrating-over-50-neural-networks-into-one-open-source-project)\n",
              "\n",
              "  3 projects\n",
              "  |\n",
              "  |dev.to](https://dev.to/radchenko/practical-experience-integrating-over-50-neural-networks-into-one-open-source-p\n",
              "roject-1jn5)\n",
              "  |\n",
              "  13 Nov 2024\n",
              "\n",
              "  &gt; Check xformers Compatibility\n",
              "  &gt; Visit the xformers GitHub repo to ensure compatibility with your torch and CUDA versions. Support for older \n",
              "versions can be dropped, so staying updated is vital, especially if you're running CUDA 11.8 and want to leverage \n",
              "xformers for limited VRAM.\n",
              "* |An Interview with AMD CEO Lisa Su About Solving Hard \n",
              "Problems](/posts/1376884-an-interview-with-amd-ceo-lisa-su-about-solving-hard-problems)\n",
              "\n",
              "  8 projects\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=40697341)\n",
              "  |\n",
              "  17 Jun 2024\n",
              "* |Animediff error](/posts/1334054-animediff-error)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |/r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/17jpsza/animediff_error/)\n",
              "  |\n",
              "  31 Oct 2023\n",
              "* |Colab | Errors when installing x-formers](/posts/1330083-colab-errors-when-installing-x-formers)\n",
              "\n",
              "  2 projects\n",
              "  |\n",
              "  |/r/comfyui](https://www.reddit.com/r/comfyui/comments/177jmtq/colab_errors_when_installing_xformers/)\n",
              "  |\n",
              "  15 Oct 2023\n",
              "\n",
              "  &gt; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This\n",
              "behaviour is the source of the following dependency conflicts.\n",
              "  &gt; fastai 2.7.12 requires torch&lt;2.1,&gt;=1.7, but you have torch 2.1.0+cu118 which is incompatible.\n",
              "  &gt; torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.1.0+cu118 which is incompatible.\n",
              "  &gt; torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.1.0+cu118 which is incompatible.\n",
              "  &gt; torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.1.0+cu118 which is incompatible.\n",
              "  &gt; torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.1.0+cu118 which is incompatible.\n",
              "  &gt; WARNING|XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
              "  &gt; PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.1.0+cu118)\n",
              "  &gt; Python 3.10.13 (you have 3.10.12)\n",
              "  &gt; Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
              "  &gt; Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
              "  &gt; Set XFORMERS\\_MORE\\_DETAILS=1 for more details\n",
              "  &gt; xformers version: 0.0.22.post3\n",
              "* |FlashAttention-2, 2x faster than FlashAttention](/posts/1308087-flashattention-2-2x-faster-than-flashattention)\n",
              "\n",
              "  3 projects\n",
              "  |\n",
              "  |news.ycombinator.com](https://news.ycombinator.com/item?id=36761988)\n",
              "  |\n",
              "  17 Jul 2023\n",
              "\n",
              "  &gt; This enables V1. V2 is still yet to be integrated into xformers. The team replied saying it should happen this \n",
              "week.\n",
              "  &gt;\n",
              "  &gt; See the relevant Github issue here: https://github.com/facebookresearch/xformers/issues/795\n",
              "* |Xformers issue](/posts/1306864-xformers-issue)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |/r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/14yef7y/xformers_issue/)\n",
              "  |\n",
              "  13 Jul 2023\n",
              "\n",
              "  &gt; My Xformers doesnt work, any help see code. info ( Exception training model: 'Refer to \n",
              "https://github.com/facebookresearch/xformers for more information on how to install xformers'. ) or\n",
              "* |Having xformer troubles](/posts/1299537-having-xformer-troubles)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |/r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/14stuvj/having_xformer_troubles/)\n",
              "  |\n",
              "  6 Jul 2023\n",
              "\n",
              "  &gt; ModuleNotFoundError: Refer to https://github.com/facebookresearch/xformers for more\n",
              "* |Question: these 4 crappy picture have been generated with the same seed and settings. Why they keep coming \n",
              "mildly \n",
              "different?](/posts/1264111-question-these-4-crappy-picture-have-been-generated-with-the-same-seed-and-settings-why-\n",
              "they-keep-coming-mildly-different)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |/r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/142thnq/question_these_4_crappy_picture_ha\n",
              "ve_been/)\n",
              "  |\n",
              "  6 Jun 2023\n",
              "\n",
              "  &gt; Xformers is a module that that can be used with Stable Diffusion. It decreases the memory required to generate \n",
              "an image as well as speeding things up. It works very well but there are two problems with Xformers:\n",
              "* |Stuck trying to update xformers](/posts/1231891-stuck-trying-to-update-xformers)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |/r/SDtechsupport](https://www.reddit.com/r/SDtechsupport/comments/13ixgky/stuck_trying_to_update_xformers/)\n",
              "  |\n",
              "  15 May 2023\n",
              "\n",
              "  &gt; WARNING|XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
              "  &gt; PyTorch 1.13.1+cu117 with CUDA 1107 (you have 2.0.1+cu118)\n",
              "  &gt; Python 3.10.9 (you have 3.10.7)\n",
              "  &gt; Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
              "  &gt; Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
              "  &gt; Set XFORMERS\\_MORE\\_DETAILS=1 for more details\n",
              "  &gt; =================================================================================\n",
              "  &gt; You are running xformers 0.0.16rc425.\n",
              "  &gt; The program is tested to work with xformers 0.0.17.\n",
              "  &gt; To reinstall the desired version, run with commandline flag --reinstall-xformers.\n",
              "  &gt; Use --skip-version-check commandline argument to disable this check.\n",
              "  &gt; =================================================================================\n",
              "* |Question about updating Xformers for A1111](/posts/1207625-question-about-updating-xformers-for-a1111)\n",
              "\n",
              "  1 project\n",
              "  |\n",
              "  |/r/SDtechsupport](https://www.reddit.com/r/SDtechsupport/comments/133a93u/question_about_updating_xformers_for_a\n",
              "1111/)\n",
              "  |\n",
              "  29 Apr 2023\n",
              "\n",
              "  &gt; # Your version of xformers is 0.0.16rc425.\n",
              "  &gt; # xformers &gt;= 0.0.17.dev is required to be available on the Dreambooth tab.\n",
              "  &gt; # Torch 1 wheels of xformers &gt;= 0.0.17.dev are no longer available on PyPI,\n",
              "  &gt; # but you can manually download them by going to:\n",
              "  &gt; https://github.com/facebookresearch/xformers/actions\n",
              "  &gt; # Click on the most recent action tagged with a release (middle column).\n",
              "  &gt; # Select a download based on your environment.\n",
              "  &gt; # Unzip your download\n",
              "  &gt; # Activate your venv and install the wheel: (from A1111 project root)\n",
              "  &gt; cd venv/Scripts\n",
              "  &gt; activate\n",
              "  &gt; pip install {REPLACE WITH PATH TO YOUR UNZIPPED .whl file}\n",
              "  &gt; # Then restart your project.\n",
              "\n",
              "What are some alternatives?\n",
              "---------------------------\n",
              "\n",
              "When comparing flash-attention and xformers you can also consider the following projects:\n",
              "\n",
              "**TensorRT**\n",
              "- NVIDIA® TensorRT™ is an SDK for high-performance deep learning inference on NVIDIA GPUs. This repository contains\n",
              "the open source components of TensorRT.\n",
              "\n",
              "**Dreambooth-Stable-Diffusion**\n",
              "- Implementation of Dreambooth (https://arxiv.org/abs/2208.12242) with Stable Diffusion\n",
              "\n",
              "**RWKV-LM**\n",
              "- RWKV (pronounced RwaKuv) is an RNN with great LLM performance, which can also be directly trained like a GPT \n",
              "transformer (parallelizable). We are at RWKV-7 \"Goose\". So it's combining the best of RNN and transformer - great \n",
              "performance, linear time, constant space (no kv-cache), fast training, infinite ctx\\_len, and free sentence \n",
              "embedding.\n",
              "\n",
              "**SHARK-Studio**\n",
              "- SHARK Studio -- Web UI for SHARK+IREE High Performance Machine Learning Distribution\n",
              "\n",
              "**DeepSpeed**\n",
              "- DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, \n",
              "and effective.\n",
              "\n",
              "**diffusers**\n",
              "- 🤗 Diffusers: State-of-the-art diffusion models for image and audio generation in PyTorch\n",
              "\n",
              "|flash-attention vs TensorRT](/compare-flash-attention-vs-TensorRT)\n",
              "|xformers vs Dreambooth-Stable-Diffusion](/compare-xformers-vs-Dreambooth-Stable-Diffusion)\n",
              "|flash-attention vs RWKV-LM](/compare-flash-attention-vs-RWKV-LM)\n",
              "|xformers vs SHARK-Studio](/compare-xformers-vs-SHARK-Studio)\n",
              "|flash-attention vs DeepSpeed](/compare-flash-attention-vs-DeepSpeed)\n",
              "|xformers vs diffusers](/compare-xformers-vs-ShivamShrirao--diffusers)\n",
              "\n",
              "!|](https://cdn-b.libhunt.com/images/promo-campaign-images/000/000/021/main.png?1663900378 \"InfluxDB Logo\")\n",
              "\n",
              "**InfluxDB – Built for High-Performance Time Series Workloads**\n",
              "\n",
              "InfluxDB 3 OSS is now GA. Transform, enrich, and act on time series data directly in the database. Automate \n",
              "critical tasks and eliminate the need to move data externally. Download now.\n",
              "\n",
              "www.influxdata.com\n",
              "\n",
              "featured\n",
              "\n",
              "!|](https://cdn-b.libhunt.com/images/promo-campaign-images/000/000/048/main.png?1753729821 \"Sevalla Logo\")\n",
              "\n",
              "**Sevalla - Deploy and host your apps and databases, now with $50 credit!**\n",
              "\n",
              "Sevalla is the PaaS you have been looking for! Advanced deployment pipelines, usage-based pricing, preview apps, \n",
              "templates, human support by developers, and much more!\n",
              "\n",
              "sevalla.com\n",
              "\n",
              "featured\n",
              "\n",
              "Do not miss the trending **Python projects** with our **weekly report!**\n",
              "\n",
              "### Did you know that **Python** is |the 2nd most popular programming language](/index#python) based on number of \n",
              "references?\n",
              "\n",
              "About\n",
              "\n",
              "LibHunt tracks mentions of software libraries on relevant social networks.\n",
              "Based on that data, you can find the most popular open-source packages,\n",
              "as well as similar and alternative projects.\n",
              "This is our **Awesome Python** list.\n",
              "\n",
              "Sponsored by\n",
              "\n",
              "|!|Sevalla logo](https://cdn-b.libhunt.com/images/promo-campaign-icons/000/000/048/icon.png?1753729822)\n",
              "Sevalla](https://sevalla.com/?utm_source=libhunt&amp;utm_medium=Referral&amp;utm_campaign=website)|!|InfluxDB \n",
              "logo](https://cdn-b.libhunt.com/images/promo-campaign-icons/000/000/021/icon.png?1678174694)\n",
              "InfluxDB](https://www.influxdata.com/products/influxdb3/?utm_source=vendor&amp;utm_medium=referral&amp;utm_campaign=2025_sp\n",
              "nsr-web_website-content_libhunt&amp;utm_content=web)|!|SaaSHub \n",
              "logo](https://cdn-b.libhunt.com/assets/partners/saashub-icon-195c856d3186551a610929bfac8bc312607331a5410cc6fb196c91\n",
              "acfb13f42a.png)\n",
              "SaaSHub](https://www.saashub.com)\n",
              "\n",
              "|Become a sponsor](/advertise)\n",
              "\n",
              "Sitemap\n",
              "\n",
              "* |About](/about)\n",
              "* |Popularity Index](/index)\n",
              "* |Site Generators](/ssg)\n",
              "* |CSS Frameworks](/css)\n",
              "* |JS Frameworks](/jsf)\n",
              "* |CMS Platforms](/cms)\n",
              "* |Databases](/dbs)\n",
              "* |/DEVs](/devs)\n",
              "* |Python Devs](/l/python/devs)\n",
              "* |RSS](https://www.libhunt.com/l/python/feed)\n",
              "\n",
              "Tools\n",
              "\n",
              "* |Add a project](/repo/submit)\n",
              "* |Find alternatives](/site/find_alternatives)\n",
              "\n",
              "Contacts\n",
              "\n",
              "* |Feedback?](/feedback)\n",
              "* |Email](/about#contacts)\n",
              "* |!|LibHunt \n",
              "badge](https://cdn-b.saashub.com/img/badges/featured-color.png?v=1)](https://www.saashub.com/libhunt?utm_source=bad\n",
              "ge&amp;badge_variant=color&amp;badge_kind=featured)\n",
              "\n",
              "LibHunt\n",
              "|\n",
              "|Privacy Policy](/privacy)\n",
              "|\n",
              "|Terms](/terms)\n",
              "|\n",
              "CC BY-SA 4.0\n",
              "\n",
              "We recommend\n",
              "|Spin The Wheel Of Names](https://spinthewheelofnames.com)\n",
              "for a cryptographically secure random name picker.\n",
              "\n",
              "Loading...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 2: Duration 2.33 seconds| Input tokens: 4,757 | Output tokens: 175]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.33 seconds| Input tokens: 4,757 | Output tokens: 175]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ Calling tool: 'final_answer' with arguments: {'answer': 'FlashAttention and xFormers are two advanced attention │\n",
              "│ mechanisms used in transformer models, each with distinct characteristics. FlashAttention is designed for speed │\n",
              "│ and memory efficiency, utilizing an IO-aware approach that minimizes memory reads and writes, resulting in      │\n",
              "│ significant performance improvements—up to 70% faster than traditional methods. It is particularly effective    │\n",
              "│ for long sequences, allowing for larger context windows without running into memory issues (Dao et al., 2022).  │\n",
              "│ In contrast, xFormers focuses on modularity and flexibility, providing a composable architecture for building   │\n",
              "│ attention mechanisms. While it also aims to reduce memory usage, it may not achieve the same level of speed as  │\n",
              "│ FlashAttention in certain scenarios (Pikoo, 2025). Overall, FlashAttention excels in performance, while         │\n",
              "│ xFormers offers greater adaptability for various applications.\\n\\n**SOURCES:**\\n1.                              │\n",
              "│ https://www.libhunt.com/compare-flash-attention-vs-xformers\\n2. https://arxiv.org/abs/2205.14135\\n3.            │\n",
              "│ https://pikoo.de/posts/flash_attention_2_is_very_good/'}                                                        │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ Calling tool: 'final_answer' with arguments: {'answer': 'FlashAttention and xFormers are two advanced attention │\n",
              "│ mechanisms used in transformer models, each with distinct characteristics. FlashAttention is designed for speed │\n",
              "│ and memory efficiency, utilizing an IO-aware approach that minimizes memory reads and writes, resulting in      │\n",
              "│ significant performance improvements—up to 70% faster than traditional methods. It is particularly effective    │\n",
              "│ for long sequences, allowing for larger context windows without running into memory issues (Dao et al., 2022).  │\n",
              "│ In contrast, xFormers focuses on modularity and flexibility, providing a composable architecture for building   │\n",
              "│ attention mechanisms. While it also aims to reduce memory usage, it may not achieve the same level of speed as  │\n",
              "│ FlashAttention in certain scenarios (Pikoo, 2025). Overall, FlashAttention excels in performance, while         │\n",
              "│ xFormers offers greater adaptability for various applications.\\n\\n**SOURCES:**\\n1.                              │\n",
              "│ https://www.libhunt.com/compare-flash-attention-vs-xformers\\n2. https://arxiv.org/abs/2205.14135\\n3.            │\n",
              "│ https://pikoo.de/posts/flash_attention_2_is_very_good/'}                                                        │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Observations: FlashAttention and xFormers are two advanced attention mechanisms used in transformer models, each \n",
              "with distinct characteristics. FlashAttention is designed for speed and memory efficiency, utilizing an IO-aware \n",
              "approach that minimizes memory reads and writes, resulting in significant performance improvements—up to 70% faster\n",
              "than traditional methods. It is particularly effective for long sequences, allowing for larger context windows \n",
              "without running into memory issues (Dao et al., 2022). In contrast, xFormers focuses on modularity and flexibility,\n",
              "providing a composable architecture for building attention mechanisms. While it also aims to reduce memory usage, \n",
              "it may not achieve the same level of speed as FlashAttention in certain scenarios (Pikoo, 2025). Overall, \n",
              "FlashAttention excels in performance, while xFormers offers greater adaptability for various applications.\n",
              "\n",
              "**SOURCES:**\n",
              "1. https://www.libhunt.com/compare-flash-attention-vs-xformers\n",
              "2. https://arxiv.org/abs/2205.14135\n",
              "3. https://pikoo.de/posts/flash_attention_2_is_very_good/\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: FlashAttention and xFormers are two advanced attention mechanisms used in transformer models, each \n",
              "with distinct characteristics. FlashAttention is designed for speed and memory efficiency, utilizing an IO-aware \n",
              "approach that minimizes memory reads and writes, resulting in significant performance improvements—up to 70% faster\n",
              "than traditional methods. It is particularly effective for long sequences, allowing for larger context windows \n",
              "without running into memory issues (Dao et al., 2022). In contrast, xFormers focuses on modularity and flexibility,\n",
              "providing a composable architecture for building attention mechanisms. While it also aims to reduce memory usage, \n",
              "it may not achieve the same level of speed as FlashAttention in certain scenarios (Pikoo, 2025). Overall, \n",
              "FlashAttention excels in performance, while xFormers offers greater adaptability for various applications.\n",
              "\n",
              "**SOURCES:**\n",
              "1. https://www.libhunt.com/compare-flash-attention-vs-xformers\n",
              "2. https://arxiv.org/abs/2205.14135\n",
              "3. https://pikoo.de/posts/flash_attention_2_is_very_good/\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;212;183;2mFinal answer: FlashAttention and xFormers are two advanced attention mechanisms used in transformer models, each \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mwith distinct characteristics. FlashAttention is designed for speed and memory efficiency, utilizing an IO-aware \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mapproach that minimizes memory reads and writes, resulting in significant performance improvements—up to 70% faster\u001b[0m\n",
              "\u001b[1;38;2;212;183;2mthan traditional methods. It is particularly effective for long sequences, allowing for larger context windows \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mwithout running into memory issues (Dao et al., 2022). In contrast, xFormers focuses on modularity and flexibility,\u001b[0m\n",
              "\u001b[1;38;2;212;183;2mproviding a composable architecture for building attention mechanisms. While it also aims to reduce memory usage, \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mit may not achieve the same level of speed as FlashAttention in certain scenarios (Pikoo, 2025). Overall, \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mFlashAttention excels in performance, while xFormers offers greater adaptability for various applications.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;212;183;2m**SOURCES:**\u001b[0m\n",
              "\u001b[1;38;2;212;183;2m1. https://www.libhunt.com/compare-flash-attention-vs-xformers\u001b[0m\n",
              "\u001b[1;38;2;212;183;2m2. https://arxiv.org/abs/2205.14135\u001b[0m\n",
              "\u001b[1;38;2;212;183;2m3. https://pikoo.de/posts/flash_attention_2_is_very_good/\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: FlashAttention and xFormers are two advanced attention mechanisms used in transformer models, each </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">with distinct characteristics. FlashAttention is designed for speed and memory efficiency, utilizing an IO-aware </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">approach that minimizes memory reads and writes, resulting in significant performance improvements—up to 70% faster</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">than traditional methods. It is particularly effective for long sequences, allowing for larger context windows </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">without running into memory issues (Dao et al., 2022). In contrast, xFormers focuses on modularity and flexibility,</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">providing a composable architecture for building attention mechanisms. While it also aims to reduce memory usage, </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">it may not achieve the same level of speed as FlashAttention in certain scenarios (Pikoo, 2025). Overall, </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">FlashAttention excels in performance, while xFormers offers greater adaptability for various applications.</span>\n",
              "\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**SOURCES:**</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">1. https://www.libhunt.com/compare-flash-attention-vs-xformers</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">2. https://arxiv.org/abs/2205.14135</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">3. https://pikoo.de/posts/flash_attention_2_is_very_good/</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 3: Duration 3.58 seconds| Input tokens: 22,746 | Output tokens: 411]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 3.58 seconds| Input tokens: 22,746 | Output tokens: 411]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FlashAttention and xFormers are two advanced attention mechanisms used in transformer models, each with distinct characteristics. FlashAttention is designed for speed and memory efficiency, utilizing an IO-aware approach that minimizes memory reads and writes, resulting in significant performance improvements—up to 70% faster than traditional methods. It is particularly effective for long sequences, allowing for larger context windows without running into memory issues (Dao et al., 2022). In contrast, xFormers focuses on modularity and flexibility, providing a composable architecture for building attention mechanisms. While it also aims to reduce memory usage, it may not achieve the same level of speed as FlashAttention in certain scenarios (Pikoo, 2025). Overall, FlashAttention excels in performance, while xFormers offers greater adaptability for various applications.\n",
            "\n",
            "**SOURCES:**\n",
            "1. https://www.libhunt.com/compare-flash-attention-vs-xformers\n",
            "2. https://arxiv.org/abs/2205.14135\n",
            "3. https://pikoo.de/posts/flash_attention_2_is_very_good/\n"
          ]
        }
      ]
    }
  ]
}